{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need to install pykeen beforehand, see https://pykeen.readthedocs.io/en/stable/installation.html \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pykeen\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyKeen comes with its own datasets that can be used directly in a pipeline.\n",
    "Below we import it so that we can explore it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romi/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pykeen.datasets import Nations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we want to be able tfo work with our own datasets as well, so we etch the online GoT dataset as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reconstructing all label-based triples. This is expensive and rarely needed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['Abelar Hightower', 'ALLIED_WITH',\n",
       "        'House Hightower of the Hightower'],\n",
       "       ['Acorn Hall', 'SEAT_OF', 'House Smallwood of Acorn Hall'],\n",
       "       ['Addam Frey', 'ALLIED_WITH', 'House Frey of the Crossing'],\n",
       "       ['Addam Marbrand', 'ALLIED_WITH', 'House Marbrand of Ashemark'],\n",
       "       ['Addam Osgrey', 'ALLIED_WITH', 'House Osgrey of Standfast']],\n",
       "      dtype='<U44')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from pykeen import triples\n",
    "\n",
    "url = 'https://ampligraph.s3-eu-west-1.amazonaws.com/datasets/GoT.csv'\n",
    "open('GoT.csv', 'wb').write(requests.get(url).content)\n",
    "\n",
    "# Format that can be read by a pd.from_csv should also be able to be read here, but the delimiter needs to be adjusted\n",
    "# PyKEEN uses tabs as defaults\n",
    "got = triples.TriplesFactory.from_path('GoT.csv',load_triples_kwargs=dict(delimiter=','))\n",
    "got_triples = got.triples\n",
    "got_triples[:5,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "List the unique subject and object entities found in the dataset. Then list all of the relationships that link the entities (note that some entities are not linked). Create an RDF version of the dataset, using your own namespaces, and save is as a ttl file. \n",
    "\n",
    "Using SPARQL, answer the following questions : \n",
    "1. How many instances per class? Use ORDER BY to show the most popular class\n",
    "2. What is the most common relation per each class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Defining train and test datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is typical in machine learning, we need to split our dataset into training and test (and sometimes validation) datasets.\n",
    "\n",
    "What differs from the standard method of randomly sampling N points to make up our test set, is that our data points are two entities linked by some relationship, and we need to take care to ensure that all entities are represented in train and test sets by at least one triple.\n",
    "\n",
    "To accomplish this, PyKEEN provides the <b>pykeen.triples.TriplesFactory.split()</b> function, which defaults to an 80/20 split. It is also by default stratified, to ensure that the distribution of the test set corresponds to that of the training set. If you want to use early stopping, you will also need a validation set. The function takes a list of percentages as argument: if you want a 95/5 split you give it <b>[0.95,0.05]</b> as argument, if you want 90/5/5 (which would include a validation set as well) you give it <b>[0.9,0.05,0.05]</b> as argument and it will return 3 datasets.\n",
    "\n",
    "For sake of example, we will create a small test size that includes only 5% of triples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using automatically assigned random_state=563294869\n",
      "Reconstructing all label-based triples. This is expensive and rarely needed.\n",
      "Reconstructing all label-based triples. This is expensive and rarely needed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size:  (3016, 3)\n",
      "Test set size:  (159, 3)\n"
     ]
    }
   ],
   "source": [
    "# got_training, got_testing = got.split()\n",
    "got_training, got_testing = got.split([0.95,0.05])\n",
    "\n",
    "print('Train set size: ', got_training.triples.shape)\n",
    "print('Test set size: ', got_testing.triples.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Create three train-test sets of different sizes from the GoT data. Give them different names. Make sure the test set is not too big when compared to the training set (test set should be max 15% of the total dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training and testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyKEEN has implemented several Knoweldge Graph Embedding models (TransE, ComplEx, DistMult, HolE, etc.). We will use the ComplEx model with default values for this tutorial.\n",
    "\n",
    "You can find the list of all implemented models in the documentation: https://pykeen.readthedocs.io/en/stable/reference/models.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing a model and instantiate it:\n",
    "There are two ways to import and use a model, both are shown below and don't give different results but not importing the model before hand might cause the automatic importing to be slower, especially if you plan to use the same model multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pykeen.utils:No cuda devices were available. The model runs on CPU\n",
      "INFO:pykeen.pipeline.api:Using device: None\n",
      "Training epochs on cpu:   0%|                                                  | 0/5 [00:00<?, ?epoch/s]\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 67.61batch/s]\u001b[A\n",
      "Training epochs on cpu:  20%|███▏            | 1/5 [00:00<00:01,  3.07epoch/s, loss=17.1, prev_loss=nan]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 79.33batch/s]\u001b[A\n",
      "Training epochs on cpu:  40%|██████         | 2/5 [00:00<00:00,  3.21epoch/s, loss=16.1, prev_loss=17.1]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 70.73batch/s]\u001b[A\n",
      "Training epochs on cpu:  60%|█████████      | 3/5 [00:00<00:00,  3.27epoch/s, loss=15.4, prev_loss=16.1]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 71.51batch/s]\u001b[A\n",
      "Training epochs on cpu:  80%|████████████   | 4/5 [00:01<00:00,  3.30epoch/s, loss=14.6, prev_loss=15.4]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 72.48batch/s]\u001b[A\n",
      "Training epochs on cpu: 100%|███████████████| 5/5 [00:01<00:00,  3.24epoch/s, loss=14.4, prev_loss=14.6]\u001b[A\n",
      "INFO:pykeen.evaluation.evaluator:Currently automatic memory optimization only supports GPUs, but you're using a CPU. Therefore, the batch_size will be set to the default value.\n",
      "INFO:pykeen.evaluation.evaluator:No evaluation batch_size provided. Setting batch_size to '32'.\n",
      "Evaluating on cpu: 100%|████████████████████████████████████████████| 159/159 [00:00<00:00, 683triple/s]\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 0.25s seconds\n",
      "WARNING:pykeen.utils:No cuda devices were available. The model runs on CPU\n",
      "INFO:pykeen.pipeline.api:Using device: None\n",
      "Training epochs on cpu:   0%|                                                  | 0/5 [00:00<?, ?epoch/s]\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 82.36batch/s]\u001b[A\n",
      "Training epochs on cpu:  20%|███▏            | 1/5 [00:00<00:01,  3.74epoch/s, loss=17.1, prev_loss=nan]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 89.91batch/s]\u001b[A\n",
      "Training epochs on cpu:  40%|██████         | 2/5 [00:00<00:00,  3.51epoch/s, loss=16.1, prev_loss=17.1]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 85.25batch/s]\u001b[A\n",
      "Training epochs on cpu:  60%|█████████      | 3/5 [00:00<00:00,  3.46epoch/s, loss=15.4, prev_loss=16.1]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 75.30batch/s]\u001b[A\n",
      "Training epochs on cpu:  80%|████████████   | 4/5 [00:01<00:00,  3.42epoch/s, loss=14.6, prev_loss=15.4]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 76.57batch/s]\u001b[A\n",
      "Training epochs on cpu: 100%|███████████████| 5/5 [00:01<00:00,  3.49epoch/s, loss=14.4, prev_loss=14.6]\u001b[A\n",
      "INFO:pykeen.evaluation.evaluator:Currently automatic memory optimization only supports GPUs, but you're using a CPU. Therefore, the batch_size will be set to the default value.\n",
      "INFO:pykeen.evaluation.evaluator:No evaluation batch_size provided. Setting batch_size to '32'.\n",
      "Evaluating on cpu: 100%|████████████████████████████████████████████| 159/159 [00:00<00:00, 687triple/s]\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 0.25s seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Losses Plot'}, xlabel='Epoch', ylabel='marginranking Loss'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wee need the pipeline to run a model, so it is simpler to import it directly.\n",
    "# Pykeen lets you train a model with the minimal amount of custom parameters\n",
    "\n",
    "from pykeen.pipeline import pipeline\n",
    "\n",
    "# here we don't import the model, but let PyKEEN do the importing.\n",
    "pipeline_result_simple = pipeline(\n",
    "    random_seed=0,\n",
    "    model='ComplEx',\n",
    "    training=got_training,\n",
    "    testing=got_testing,\n",
    ")\n",
    "pipeline_result_imported.plot_losses()\n",
    "\n",
    "# here we import the model and use it directly.\n",
    "from pykeen.models import ComplEx\n",
    "\n",
    "pipeline_result_imported = pipeline(\n",
    "    random_seed=0,\n",
    "    model=ComplEx,\n",
    "    training=got_training,\n",
    "    testing=got_testing,\n",
    ")\n",
    "pipeline_result_imported.plot_losses()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can retrieve different metrics from the results. Here we retrieve the mean reciprocal rank (MRR). The result is the same for both the simple and imported model, because we used the same random seed (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0029104012738233845\n",
      "0.0029104012738233845\n"
     ]
    }
   ],
   "source": [
    "print(pipeline_result_imported.get_metric('mrr'))\n",
    "print(pipeline_result_simple.get_metric('mrr'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pykeen.utils:No cuda devices were available. The model runs on CPU\n",
      "INFO:pykeen.pipeline.api:Using device: None\n",
      "Training epochs on cpu:   0%|                                                | 0/200 [00:00<?, ?epoch/s]\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 61.86batch/s]\u001b[A\n",
      "Training epochs on cpu:   0%|                | 1/200 [00:00<01:07,  2.94epoch/s, loss=17, prev_loss=nan]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 68.11batch/s]\u001b[A\n",
      "Training epochs on cpu:   1%|▏                | 2/200 [00:00<01:02,  3.18epoch/s, loss=16, prev_loss=17]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 58.82batch/s]\u001b[A\n",
      "Training epochs on cpu:   2%|▏              | 3/200 [00:00<01:04,  3.04epoch/s, loss=15.3, prev_loss=16]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 64.67batch/s]\u001b[A\n",
      "Training epochs on cpu:   2%|▎            | 4/200 [00:01<01:05,  3.01epoch/s, loss=14.6, prev_loss=15.3]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 62.98batch/s]\u001b[A\n",
      "Training epochs on cpu:   2%|▎            | 5/200 [00:01<01:05,  2.99epoch/s, loss=14.3, prev_loss=14.6]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 64.87batch/s]\u001b[A\n",
      "Training epochs on cpu:   3%|▍            | 6/200 [00:01<01:02,  3.10epoch/s, loss=13.4, prev_loss=14.3]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 60.80batch/s]\u001b[A\n",
      "Training epochs on cpu:   4%|▍            | 7/200 [00:02<01:03,  3.03epoch/s, loss=12.7, prev_loss=13.4]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 63.82batch/s]\u001b[A\n",
      "Training epochs on cpu:   4%|▌            | 8/200 [00:02<01:03,  3.01epoch/s, loss=12.3, prev_loss=12.7]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 71.79batch/s]\u001b[A\n",
      "Training epochs on cpu:   4%|▌            | 9/200 [00:02<01:01,  3.11epoch/s, loss=11.4, prev_loss=12.3]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 65.34batch/s]\u001b[A\n",
      "Training epochs on cpu:   5%|▌           | 10/200 [00:03<01:01,  3.08epoch/s, loss=11.2, prev_loss=11.4]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 61.86batch/s]\u001b[A\n",
      "Training epochs on cpu:   6%|▋           | 11/200 [00:03<01:01,  3.05epoch/s, loss=10.7, prev_loss=11.2]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 62.94batch/s]\u001b[A\n",
      "Training epochs on cpu:   6%|▋           | 12/200 [00:03<01:01,  3.07epoch/s, loss=10.6, prev_loss=10.7]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 57.95batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 59.02batch/s]\u001b[A\n",
      "Training epochs on cpu:   6%|▊           | 13/200 [00:04<01:02,  2.98epoch/s, loss=9.72, prev_loss=10.6]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 60.61batch/s]\u001b[A\n",
      "Training epochs on cpu:   7%|▊           | 14/200 [00:04<01:02,  2.97epoch/s, loss=9.68, prev_loss=9.72]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 62.08batch/s]\u001b[A\n",
      "Training epochs on cpu:   8%|▉           | 15/200 [00:04<01:01,  2.99epoch/s, loss=9.24, prev_loss=9.68]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 61.87batch/s]\u001b[A\n",
      "Training epochs on cpu:   8%|▉           | 16/200 [00:05<01:02,  2.97epoch/s, loss=8.59, prev_loss=9.24]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 62.54batch/s]\u001b[A\n",
      "Training epochs on cpu:   8%|█           | 17/200 [00:05<01:01,  2.96epoch/s, loss=8.11, prev_loss=8.59]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 62.14batch/s]\u001b[A\n",
      "Training epochs on cpu:   9%|█           | 18/200 [00:05<01:01,  2.98epoch/s, loss=7.86, prev_loss=8.11]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.53batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 59.80batch/s]\u001b[A\n",
      "Training epochs on cpu:  10%|█▏           | 19/200 [00:06<01:01,  2.96epoch/s, loss=7.7, prev_loss=7.86]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 60.29batch/s]\u001b[A\n",
      "Training epochs on cpu:  10%|█▎           | 20/200 [00:06<01:01,  2.95epoch/s, loss=7.49, prev_loss=7.7]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.87batch/s]\u001b[A\n",
      "Training epochs on cpu:  10%|█▎          | 21/200 [00:06<01:01,  2.93epoch/s, loss=7.08, prev_loss=7.49]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 63.08batch/s]\u001b[A\n",
      "Training epochs on cpu:  11%|█▎          | 22/200 [00:07<01:00,  2.92epoch/s, loss=7.02, prev_loss=7.08]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.17batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 58.62batch/s]\u001b[A\n",
      "Training epochs on cpu:  12%|█▍          | 23/200 [00:07<01:00,  2.90epoch/s, loss=6.24, prev_loss=7.02]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 61.05batch/s]\u001b[A\n",
      "Training epochs on cpu:  12%|█▌           | 24/200 [00:08<00:59,  2.95epoch/s, loss=6.1, prev_loss=6.24]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 61.74batch/s]\u001b[A\n",
      "Training epochs on cpu:  12%|█▋           | 25/200 [00:08<00:58,  3.01epoch/s, loss=5.95, prev_loss=6.1]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 61.96batch/s]\u001b[A\n",
      "Training epochs on cpu:  13%|█▌          | 26/200 [00:08<00:58,  2.98epoch/s, loss=5.84, prev_loss=5.95]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 57.08batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 56.89batch/s]\u001b[A\n",
      "Training epochs on cpu:  14%|█▌          | 27/200 [00:09<00:59,  2.92epoch/s, loss=5.56, prev_loss=5.84]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.54batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 59.12batch/s]\u001b[A\n",
      "Training epochs on cpu:  14%|█▋          | 28/200 [00:09<00:59,  2.90epoch/s, loss=5.47, prev_loss=5.56]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.16batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 57.87batch/s]\u001b[A\n",
      "Training epochs on cpu:  14%|█▋          | 29/200 [00:09<00:58,  2.94epoch/s, loss=5.15, prev_loss=5.47]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 56.08batch/s]\u001b[A\n",
      "Training epochs on cpu:  15%|█▊          | 30/200 [00:10<00:58,  2.91epoch/s, loss=4.95, prev_loss=5.15]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 69.30batch/s]\u001b[A\n",
      "Training epochs on cpu:  16%|█▊          | 31/200 [00:10<00:55,  3.02epoch/s, loss=4.44, prev_loss=4.95]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 66.57batch/s]\u001b[A\n",
      "Training epochs on cpu:  16%|█▉          | 32/200 [00:10<00:54,  3.08epoch/s, loss=4.48, prev_loss=4.44]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 58.77batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 57.60batch/s]\u001b[A\n",
      "Training epochs on cpu:  16%|█▉          | 33/200 [00:11<00:55,  3.00epoch/s, loss=4.47, prev_loss=4.48]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 60.18batch/s]\u001b[A\n",
      "Training epochs on cpu:  17%|██          | 34/200 [00:11<00:54,  3.05epoch/s, loss=4.39, prev_loss=4.47]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 63.42batch/s]\u001b[A\n",
      "Training epochs on cpu:  18%|██          | 35/200 [00:11<00:54,  3.02epoch/s, loss=3.93, prev_loss=4.39]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 56.74batch/s]\u001b[A\n",
      "Training epochs on cpu:  18%|██▏         | 36/200 [00:12<00:54,  3.03epoch/s, loss=3.84, prev_loss=3.93]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 60.43batch/s]\u001b[A\n",
      "Training epochs on cpu:  18%|██▏         | 37/200 [00:12<00:54,  2.97epoch/s, loss=3.79, prev_loss=3.84]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 68.78batch/s]\u001b[A\n",
      "Training epochs on cpu:  19%|██▎         | 38/200 [00:12<00:53,  3.04epoch/s, loss=3.76, prev_loss=3.79]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 61.76batch/s]\u001b[A\n",
      "Training epochs on cpu:  20%|██▎         | 39/200 [00:13<00:53,  3.00epoch/s, loss=3.44, prev_loss=3.76]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 62.41batch/s]\u001b[A\n",
      "Training epochs on cpu:  20%|██▍         | 40/200 [00:13<00:53,  3.00epoch/s, loss=3.45, prev_loss=3.44]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 70.39batch/s]\u001b[A\n",
      "Training epochs on cpu:  20%|██▍         | 41/200 [00:13<00:52,  3.03epoch/s, loss=3.17, prev_loss=3.45]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 60.74batch/s]\u001b[A\n",
      "Training epochs on cpu:  21%|██▌         | 42/200 [00:14<00:52,  3.01epoch/s, loss=3.04, prev_loss=3.17]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.06batch/s]\u001b[A\n",
      "Training epochs on cpu:  22%|██▌         | 43/200 [00:14<00:51,  3.07epoch/s, loss=3.09, prev_loss=3.04]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 56.96batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 57.06batch/s]\u001b[A\n",
      "Training epochs on cpu:  22%|██▋         | 44/200 [00:14<00:52,  2.98epoch/s, loss=2.88, prev_loss=3.09]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 69.31batch/s]\u001b[A\n",
      "Training epochs on cpu:  22%|██▋         | 45/200 [00:14<00:50,  3.10epoch/s, loss=2.66, prev_loss=2.88]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 56.78batch/s]\u001b[A\n",
      "Training epochs on cpu:  23%|██▊         | 46/200 [00:15<00:49,  3.09epoch/s, loss=2.61, prev_loss=2.66]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 68.69batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs on cpu:  24%|██▊         | 47/200 [00:15<00:48,  3.14epoch/s, loss=2.76, prev_loss=2.61]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 65.88batch/s]\u001b[A\n",
      "Training epochs on cpu:  24%|██▉         | 48/200 [00:15<00:47,  3.17epoch/s, loss=2.82, prev_loss=2.76]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 60.34batch/s]\u001b[A\n",
      "Training epochs on cpu:  24%|███▏         | 49/200 [00:16<00:49,  3.06epoch/s, loss=2.4, prev_loss=2.82]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 61.88batch/s]\u001b[A\n",
      "Training epochs on cpu:  25%|███▎         | 50/200 [00:16<00:49,  3.03epoch/s, loss=2.57, prev_loss=2.4]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 64.45batch/s]\u001b[A\n",
      "Training epochs on cpu:  26%|███         | 51/200 [00:16<00:49,  2.99epoch/s, loss=2.26, prev_loss=2.57]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 57.55batch/s]\u001b[A\n",
      "Training epochs on cpu:  26%|███         | 52/200 [00:17<00:48,  3.05epoch/s, loss=2.16, prev_loss=2.26]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 68.83batch/s]\u001b[A\n",
      "Training epochs on cpu:  26%|███▍         | 53/200 [00:17<00:48,  3.05epoch/s, loss=2.1, prev_loss=2.16]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 61.70batch/s]\u001b[A\n",
      "Training epochs on cpu:  27%|███▌         | 54/200 [00:17<00:48,  3.01epoch/s, loss=2.07, prev_loss=2.1]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 64.01batch/s]\u001b[A\n",
      "Training epochs on cpu:  28%|███▎        | 55/200 [00:18<00:46,  3.09epoch/s, loss=2.08, prev_loss=2.07]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 60.25batch/s]\u001b[A\n",
      "Training epochs on cpu:  28%|███▎        | 56/200 [00:18<00:47,  3.04epoch/s, loss=2.05, prev_loss=2.08]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 72.41batch/s]\u001b[A\n",
      "Training epochs on cpu:  28%|███▍        | 57/200 [00:18<00:45,  3.15epoch/s, loss=1.75, prev_loss=2.05]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 58.74batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 59.32batch/s]\u001b[A\n",
      "Training epochs on cpu:  29%|███▍        | 58/200 [00:19<00:46,  3.06epoch/s, loss=1.75, prev_loss=1.75]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 68.72batch/s]\u001b[A\n",
      "Training epochs on cpu:  30%|███▌        | 59/200 [00:19<00:44,  3.14epoch/s, loss=1.75, prev_loss=1.75]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 69.29batch/s]\u001b[A\n",
      "Training epochs on cpu:  30%|███▌        | 60/200 [00:19<00:44,  3.15epoch/s, loss=1.67, prev_loss=1.75]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 62.38batch/s]\u001b[A\n",
      "Training epochs on cpu:  30%|███▋        | 61/200 [00:20<00:44,  3.09epoch/s, loss=1.71, prev_loss=1.67]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 62.43batch/s]\u001b[A\n",
      "Training epochs on cpu:  31%|███▋        | 62/200 [00:20<00:44,  3.11epoch/s, loss=1.58, prev_loss=1.71]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 63.97batch/s]\u001b[A\n",
      "Training epochs on cpu:  32%|███▊        | 63/200 [00:20<00:44,  3.05epoch/s, loss=1.49, prev_loss=1.58]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 68.63batch/s]\u001b[A\n",
      "Training epochs on cpu:  32%|███▊        | 64/200 [00:21<00:44,  3.03epoch/s, loss=1.61, prev_loss=1.49]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 69.38batch/s]\u001b[A\n",
      "Training epochs on cpu:  32%|███▉        | 65/200 [00:21<00:44,  3.06epoch/s, loss=1.53, prev_loss=1.61]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 62.49batch/s]\u001b[A\n",
      "Training epochs on cpu:  33%|███▉        | 66/200 [00:21<00:44,  3.03epoch/s, loss=1.59, prev_loss=1.53]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 62.87batch/s]\u001b[A\n",
      "Training epochs on cpu:  34%|████        | 67/200 [00:22<00:44,  3.00epoch/s, loss=1.53, prev_loss=1.59]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 61.98batch/s]\u001b[A\n",
      "Training epochs on cpu:  34%|████        | 68/200 [00:22<00:43,  3.07epoch/s, loss=1.49, prev_loss=1.53]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 64.01batch/s]\u001b[A\n",
      "Training epochs on cpu:  34%|████▏       | 69/200 [00:22<00:43,  3.04epoch/s, loss=1.37, prev_loss=1.49]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.72batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 58.99batch/s]\u001b[A\n",
      "Training epochs on cpu:  35%|████▏       | 70/200 [00:23<00:43,  2.98epoch/s, loss=1.38, prev_loss=1.37]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 71.34batch/s]\u001b[A\n",
      "Training epochs on cpu:  36%|████▎       | 71/200 [00:23<00:42,  3.06epoch/s, loss=1.39, prev_loss=1.38]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 60.69batch/s]\u001b[A\n",
      "Training epochs on cpu:  36%|████▎       | 72/200 [00:23<00:42,  3.02epoch/s, loss=1.14, prev_loss=1.39]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.88batch/s]\u001b[A\n",
      "Training epochs on cpu:  36%|████▍       | 73/200 [00:24<00:42,  3.00epoch/s, loss=1.18, prev_loss=1.14]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.19batch/s]\u001b[A\n",
      "Training epochs on cpu:  37%|████▍       | 74/200 [00:24<00:42,  2.97epoch/s, loss=1.16, prev_loss=1.18]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 66.77batch/s]\u001b[A\n",
      "Training epochs on cpu:  38%|████▌       | 75/200 [00:24<00:40,  3.06epoch/s, loss=1.13, prev_loss=1.16]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 58.28batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 58.35batch/s]\u001b[A\n",
      "Training epochs on cpu:  38%|████▌       | 76/200 [00:25<00:41,  2.98epoch/s, loss=1.12, prev_loss=1.13]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 65.48batch/s]\u001b[A\n",
      "Training epochs on cpu:  38%|████▌       | 77/200 [00:25<00:39,  3.08epoch/s, loss=1.06, prev_loss=1.12]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.14batch/s]\u001b[A\n",
      "Training epochs on cpu:  39%|████▋       | 78/200 [00:25<00:40,  3.05epoch/s, loss=1.06, prev_loss=1.06]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 58.27batch/s]\u001b[A\n",
      "Training epochs on cpu:  40%|████▋       | 79/200 [00:26<00:40,  3.01epoch/s, loss=1.07, prev_loss=1.06]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 62.44batch/s]\u001b[A\n",
      "Training epochs on cpu:  40%|████▊       | 80/200 [00:26<00:40,  2.99epoch/s, loss=1.03, prev_loss=1.07]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 64.58batch/s]\u001b[A\n",
      "Training epochs on cpu:  40%|████▊       | 81/200 [00:26<00:39,  3.00epoch/s, loss=1.01, prev_loss=1.03]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 68.32batch/s]\u001b[A\n",
      "Training epochs on cpu:  41%|████▌      | 82/200 [00:27<00:39,  3.02epoch/s, loss=0.985, prev_loss=1.01]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 62.86batch/s]\u001b[A\n",
      "Training epochs on cpu:  42%|████▏     | 83/200 [00:27<00:38,  3.08epoch/s, loss=0.971, prev_loss=0.985]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 66.88batch/s]\u001b[A\n",
      "Training epochs on cpu:  42%|████▏     | 84/200 [00:27<00:37,  3.06epoch/s, loss=0.858, prev_loss=0.971]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 67.11batch/s]\u001b[A\n",
      "Training epochs on cpu:  42%|████▎     | 85/200 [00:28<00:37,  3.05epoch/s, loss=0.885, prev_loss=0.858]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 57.15batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 56.88batch/s]\u001b[A\n",
      "Training epochs on cpu:  43%|████▎     | 86/200 [00:28<00:38,  2.98epoch/s, loss=0.836, prev_loss=0.885]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 67.37batch/s]\u001b[A\n",
      "Training epochs on cpu:  44%|████▎     | 87/200 [00:28<00:37,  2.99epoch/s, loss=0.861, prev_loss=0.836]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 58.17batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 57.62batch/s]\u001b[A\n",
      "Training epochs on cpu:  44%|████▍     | 88/200 [00:29<00:37,  2.98epoch/s, loss=0.796, prev_loss=0.861]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 66.92batch/s]\u001b[A\n",
      "Training epochs on cpu:  44%|████▍     | 89/200 [00:29<00:36,  3.02epoch/s, loss=0.738, prev_loss=0.796]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 66.28batch/s]\u001b[A\n",
      "Training epochs on cpu:  45%|████▌     | 90/200 [00:29<00:35,  3.10epoch/s, loss=0.708, prev_loss=0.738]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 55.95batch/s]\u001b[A\n",
      "Training epochs on cpu:  46%|████▌     | 91/200 [00:30<00:36,  3.03epoch/s, loss=0.894, prev_loss=0.708]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 62.32batch/s]\u001b[A\n",
      "Training epochs on cpu:  46%|████▌     | 92/200 [00:30<00:35,  3.08epoch/s, loss=0.711, prev_loss=0.894]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 57.91batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 58.03batch/s]\u001b[A\n",
      "Training epochs on cpu:  46%|████▋     | 93/200 [00:30<00:34,  3.07epoch/s, loss=0.715, prev_loss=0.711]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 57.87batch/s]\u001b[A\n",
      "Training epochs on cpu:  47%|█████▏     | 94/200 [00:31<00:34,  3.05epoch/s, loss=0.64, prev_loss=0.715]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 58.23batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs on cpu:  48%|█████▏     | 95/200 [00:31<00:35,  3.00epoch/s, loss=0.737, prev_loss=0.64]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 66.31batch/s]\u001b[A\n",
      "Training epochs on cpu:  48%|████▊     | 96/200 [00:31<00:34,  2.99epoch/s, loss=0.692, prev_loss=0.737]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 61.90batch/s]\u001b[A\n",
      "Training epochs on cpu:  48%|████▊     | 97/200 [00:32<00:34,  2.98epoch/s, loss=0.707, prev_loss=0.692]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 70.40batch/s]\u001b[A\n",
      "Training epochs on cpu:  49%|████▉     | 98/200 [00:32<00:33,  3.00epoch/s, loss=0.652, prev_loss=0.707]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 63.87batch/s]\u001b[A\n",
      "Training epochs on cpu:  50%|████▉     | 99/200 [00:32<00:33,  3.00epoch/s, loss=0.611, prev_loss=0.652]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 63.31batch/s]\u001b[A\n",
      "Training epochs on cpu:  50%|█████     | 100/200 [00:33<00:33,  3.01epoch/s, loss=0.66, prev_loss=0.611]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 65.44batch/s]\u001b[A\n",
      "Training epochs on cpu:  50%|█████     | 101/200 [00:33<00:33,  2.99epoch/s, loss=0.561, prev_loss=0.66]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 62.80batch/s]\u001b[A\n",
      "Training epochs on cpu:  51%|████▌    | 102/200 [00:33<00:32,  2.98epoch/s, loss=0.578, prev_loss=0.561]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.76batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 59.46batch/s]\u001b[A\n",
      "Training epochs on cpu:  52%|████▋    | 103/200 [00:34<00:32,  3.00epoch/s, loss=0.599, prev_loss=0.578]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 67.89batch/s]\u001b[A\n",
      "Training epochs on cpu:  52%|████▋    | 104/200 [00:34<00:31,  3.08epoch/s, loss=0.569, prev_loss=0.599]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 60.40batch/s]\u001b[A\n",
      "Training epochs on cpu:  52%|████▋    | 105/200 [00:34<00:31,  3.03epoch/s, loss=0.578, prev_loss=0.569]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 62.35batch/s]\u001b[A\n",
      "Training epochs on cpu:  53%|████▊    | 106/200 [00:35<00:31,  3.01epoch/s, loss=0.661, prev_loss=0.578]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 60.94batch/s]\u001b[A\n",
      "Training epochs on cpu:  54%|█████▎    | 107/200 [00:35<00:30,  3.00epoch/s, loss=0.59, prev_loss=0.661]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 61.19batch/s]\u001b[A\n",
      "Training epochs on cpu:  54%|█████▍    | 108/200 [00:35<00:30,  3.07epoch/s, loss=0.454, prev_loss=0.59]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 62.20batch/s]\u001b[A\n",
      "Training epochs on cpu:  55%|████▉    | 109/200 [00:36<00:30,  3.03epoch/s, loss=0.531, prev_loss=0.454]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 65.08batch/s]\u001b[A\n",
      "Training epochs on cpu:  55%|████▉    | 110/200 [00:36<00:29,  3.10epoch/s, loss=0.537, prev_loss=0.531]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 63.43batch/s]\u001b[A\n",
      "Training epochs on cpu:  56%|████▉    | 111/200 [00:36<00:28,  3.07epoch/s, loss=0.558, prev_loss=0.537]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 63.62batch/s]\u001b[A\n",
      "Training epochs on cpu:  56%|█████    | 112/200 [00:37<00:28,  3.04epoch/s, loss=0.529, prev_loss=0.558]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 60.57batch/s]\u001b[A\n",
      "Training epochs on cpu:  56%|█████    | 113/200 [00:37<00:29,  2.99epoch/s, loss=0.501, prev_loss=0.529]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 60.91batch/s]\u001b[A\n",
      "Training epochs on cpu:  57%|█████▏   | 114/200 [00:37<00:29,  2.96epoch/s, loss=0.389, prev_loss=0.501]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 68.16batch/s]\u001b[A\n",
      "Training epochs on cpu:  57%|█████▏   | 115/200 [00:38<00:28,  2.96epoch/s, loss=0.495, prev_loss=0.389]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 57.52batch/s]\u001b[A\n",
      "Training epochs on cpu:  58%|█████▏   | 116/200 [00:38<00:27,  3.01epoch/s, loss=0.536, prev_loss=0.495]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 63.84batch/s]\u001b[A\n",
      "Training epochs on cpu:  58%|█████▎   | 117/200 [00:38<00:27,  3.02epoch/s, loss=0.441, prev_loss=0.536]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 67.25batch/s]\u001b[A\n",
      "Training epochs on cpu:  59%|█████▎   | 118/200 [00:39<00:27,  3.02epoch/s, loss=0.461, prev_loss=0.441]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 66.92batch/s]\u001b[A\n",
      "Training epochs on cpu:  60%|█████▎   | 119/200 [00:39<00:26,  3.01epoch/s, loss=0.441, prev_loss=0.461]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 61.57batch/s]\u001b[A\n",
      "Training epochs on cpu:  60%|█████▍   | 120/200 [00:39<00:26,  2.99epoch/s, loss=0.393, prev_loss=0.441]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 63.39batch/s]\u001b[A\n",
      "Training epochs on cpu:  60%|█████▍   | 121/200 [00:40<00:26,  3.03epoch/s, loss=0.384, prev_loss=0.393]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 58.05batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 59.11batch/s]\u001b[A\n",
      "Training epochs on cpu:  61%|█████▍   | 122/200 [00:40<00:25,  3.01epoch/s, loss=0.384, prev_loss=0.384]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 66.46batch/s]\u001b[A\n",
      "Training epochs on cpu:  62%|█████▌   | 123/200 [00:40<00:25,  3.07epoch/s, loss=0.424, prev_loss=0.384]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 58.43batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 58.15batch/s]\u001b[A\n",
      "Training epochs on cpu:  62%|█████▌   | 124/200 [00:41<00:25,  3.00epoch/s, loss=0.406, prev_loss=0.424]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 64.80batch/s]\u001b[A\n",
      "Training epochs on cpu:  62%|█████▋   | 125/200 [00:41<00:24,  3.06epoch/s, loss=0.442, prev_loss=0.406]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 60.73batch/s]\u001b[A\n",
      "Training epochs on cpu:  63%|█████▋   | 126/200 [00:41<00:24,  3.02epoch/s, loss=0.462, prev_loss=0.442]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 69.68batch/s]\u001b[A\n",
      "Training epochs on cpu:  64%|█████▋   | 127/200 [00:41<00:23,  3.13epoch/s, loss=0.382, prev_loss=0.462]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 68.51batch/s]\u001b[A\n",
      "Training epochs on cpu:  64%|█████▊   | 128/200 [00:42<00:23,  3.12epoch/s, loss=0.397, prev_loss=0.382]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 61.08batch/s]\u001b[A\n",
      "Training epochs on cpu:  64%|█████▊   | 129/200 [00:42<00:23,  3.06epoch/s, loss=0.418, prev_loss=0.397]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 69.35batch/s]\u001b[A\n",
      "Training epochs on cpu:  65%|██████▌   | 130/200 [00:42<00:22,  3.09epoch/s, loss=0.29, prev_loss=0.418]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 61.80batch/s]\u001b[A\n",
      "Training epochs on cpu:  66%|██████▌   | 131/200 [00:43<00:22,  3.11epoch/s, loss=0.335, prev_loss=0.29]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 62.16batch/s]\u001b[A\n",
      "Training epochs on cpu:  66%|█████▉   | 132/200 [00:43<00:22,  3.07epoch/s, loss=0.356, prev_loss=0.335]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 66.20batch/s]\u001b[A\n",
      "Training epochs on cpu:  66%|██████▋   | 133/200 [00:43<00:21,  3.08epoch/s, loss=0.32, prev_loss=0.356]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 63.26batch/s]\u001b[A\n",
      "Training epochs on cpu:  67%|██████▋   | 134/200 [00:44<00:20,  3.15epoch/s, loss=0.351, prev_loss=0.32]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 68.89batch/s]\u001b[A\n",
      "Training epochs on cpu:  68%|███████▍   | 135/200 [00:44<00:20,  3.11epoch/s, loss=0.3, prev_loss=0.351]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 63.52batch/s]\u001b[A\n",
      "Training epochs on cpu:  68%|███████▍   | 136/200 [00:44<00:20,  3.16epoch/s, loss=0.329, prev_loss=0.3]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 58.05batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 57.25batch/s]\u001b[A\n",
      "Training epochs on cpu:  68%|██████▏  | 137/200 [00:45<00:20,  3.04epoch/s, loss=0.347, prev_loss=0.329]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 61.56batch/s]\u001b[A\n",
      "Training epochs on cpu:  69%|██████▏  | 138/200 [00:45<00:20,  3.00epoch/s, loss=0.349, prev_loss=0.347]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 67.04batch/s]\u001b[A\n",
      "Training epochs on cpu:  70%|██████▎  | 139/200 [00:45<00:19,  3.07epoch/s, loss=0.319, prev_loss=0.349]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.27batch/s]\u001b[A\n",
      "Training epochs on cpu:  70%|██████▎  | 140/200 [00:46<00:19,  3.07epoch/s, loss=0.323, prev_loss=0.319]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.09batch/s]\u001b[A\n",
      "Training epochs on cpu:  70%|██████▎  | 141/200 [00:46<00:19,  3.04epoch/s, loss=0.363, prev_loss=0.323]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 70.83batch/s]\u001b[A\n",
      "Training epochs on cpu:  71%|██████▍  | 142/200 [00:46<00:19,  3.05epoch/s, loss=0.332, prev_loss=0.363]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 61.72batch/s]\u001b[A\n",
      "Training epochs on cpu:  72%|██████▍  | 143/200 [00:47<00:18,  3.02epoch/s, loss=0.286, prev_loss=0.332]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 66.57batch/s]\u001b[A\n",
      "Training epochs on cpu:  72%|██████▍  | 144/200 [00:47<00:17,  3.12epoch/s, loss=0.326, prev_loss=0.286]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 61.84batch/s]\u001b[A\n",
      "Training epochs on cpu:  72%|██████▌  | 145/200 [00:47<00:17,  3.06epoch/s, loss=0.262, prev_loss=0.326]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 65.02batch/s]\u001b[A\n",
      "Training epochs on cpu:  73%|██████▌  | 146/200 [00:48<00:17,  3.09epoch/s, loss=0.234, prev_loss=0.262]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 64.40batch/s]\u001b[A\n",
      "Training epochs on cpu:  74%|██████▌  | 147/200 [00:48<00:17,  3.04epoch/s, loss=0.293, prev_loss=0.234]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 69.76batch/s]\u001b[A\n",
      "Training epochs on cpu:  74%|██████▋  | 148/200 [00:48<00:16,  3.10epoch/s, loss=0.304, prev_loss=0.293]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 65.03batch/s]\u001b[A\n",
      "Training epochs on cpu:  74%|██████▋  | 149/200 [00:49<00:16,  3.11epoch/s, loss=0.314, prev_loss=0.304]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 61.39batch/s]\u001b[A\n",
      "Training epochs on cpu:  75%|██████▊  | 150/200 [00:49<00:16,  3.03epoch/s, loss=0.273, prev_loss=0.314]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.25batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 58.99batch/s]\u001b[A\n",
      "Training epochs on cpu:  76%|██████▊  | 151/200 [00:49<00:16,  2.98epoch/s, loss=0.282, prev_loss=0.273]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 57.93batch/s]\u001b[A\n",
      "Training epochs on cpu:  76%|██████▊  | 152/200 [00:50<00:16,  2.96epoch/s, loss=0.234, prev_loss=0.282]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 60.97batch/s]\u001b[A\n",
      "Training epochs on cpu:  76%|██████▉  | 153/200 [00:50<00:15,  2.94epoch/s, loss=0.278, prev_loss=0.234]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 62.09batch/s]\u001b[A\n",
      "Training epochs on cpu:  77%|██████▉  | 154/200 [00:50<00:15,  2.93epoch/s, loss=0.261, prev_loss=0.278]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.11batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 58.68batch/s]\u001b[A\n",
      "Training epochs on cpu:  78%|██████▉  | 155/200 [00:51<00:15,  2.91epoch/s, loss=0.219, prev_loss=0.261]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 60.38batch/s]\u001b[A\n",
      "Training epochs on cpu:  78%|███████  | 156/200 [00:51<00:15,  2.91epoch/s, loss=0.307, prev_loss=0.219]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.82batch/s]\u001b[A\n",
      "Training epochs on cpu:  78%|███████  | 157/200 [00:51<00:14,  2.97epoch/s, loss=0.262, prev_loss=0.307]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 66.94batch/s]\u001b[A\n",
      "Training epochs on cpu:  79%|███████  | 158/200 [00:52<00:13,  3.08epoch/s, loss=0.278, prev_loss=0.262]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 63.99batch/s]\u001b[A\n",
      "Training epochs on cpu:  80%|███████▏ | 159/200 [00:52<00:13,  3.14epoch/s, loss=0.197, prev_loss=0.278]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.69batch/s]\u001b[A\n",
      "Training epochs on cpu:  80%|███████▏ | 160/200 [00:52<00:12,  3.17epoch/s, loss=0.227, prev_loss=0.197]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 56.39batch/s]\u001b[A\n",
      "Training epochs on cpu:  80%|███████▏ | 161/200 [00:53<00:12,  3.11epoch/s, loss=0.237, prev_loss=0.227]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 60.53batch/s]\u001b[A\n",
      "Training epochs on cpu:  81%|███████▎ | 162/200 [00:53<00:12,  3.06epoch/s, loss=0.272, prev_loss=0.237]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 62.38batch/s]\u001b[A\n",
      "Training epochs on cpu:  82%|███████▎ | 163/200 [00:53<00:11,  3.11epoch/s, loss=0.232, prev_loss=0.272]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 58.31batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 58.31batch/s]\u001b[A\n",
      "Training epochs on cpu:  82%|███████▍ | 164/200 [00:54<00:11,  3.03epoch/s, loss=0.209, prev_loss=0.232]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 61.25batch/s]\u001b[A\n",
      "Training epochs on cpu:  82%|███████▍ | 165/200 [00:54<00:11,  2.98epoch/s, loss=0.198, prev_loss=0.209]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.82batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 58.69batch/s]\u001b[A\n",
      "Training epochs on cpu:  83%|████████▎ | 166/200 [00:54<00:11,  2.95epoch/s, loss=0.23, prev_loss=0.198]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.20batch/s]\u001b[A\n",
      "Training epochs on cpu:  84%|████████▎ | 167/200 [00:55<00:11,  2.92epoch/s, loss=0.215, prev_loss=0.23]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 60.34batch/s]\u001b[A\n",
      "Training epochs on cpu:  84%|███████▌ | 168/200 [00:55<00:10,  2.91epoch/s, loss=0.225, prev_loss=0.215]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 62.28batch/s]\u001b[A\n",
      "Training epochs on cpu:  84%|███████▌ | 169/200 [00:55<00:10,  2.95epoch/s, loss=0.215, prev_loss=0.225]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.64batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 59.40batch/s]\u001b[A\n",
      "Training epochs on cpu:  85%|███████▋ | 170/200 [00:56<00:10,  2.92epoch/s, loss=0.227, prev_loss=0.215]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 62.07batch/s]\u001b[A\n",
      "Training epochs on cpu:  86%|███████▋ | 171/200 [00:56<00:09,  3.00epoch/s, loss=0.187, prev_loss=0.227]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 67.84batch/s]\u001b[A\n",
      "Training epochs on cpu:  86%|███████▋ | 172/200 [00:56<00:09,  3.02epoch/s, loss=0.187, prev_loss=0.187]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 61.81batch/s]\u001b[A\n",
      "Training epochs on cpu:  86%|███████▊ | 173/200 [00:57<00:08,  3.01epoch/s, loss=0.186, prev_loss=0.187]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 63.57batch/s]\u001b[A\n",
      "Training epochs on cpu:  87%|███████▊ | 174/200 [00:57<00:08,  3.00epoch/s, loss=0.226, prev_loss=0.186]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 71.50batch/s]\u001b[A\n",
      "Training epochs on cpu:  88%|███████▉ | 175/200 [00:57<00:08,  3.04epoch/s, loss=0.209, prev_loss=0.226]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 68.86batch/s]\u001b[A\n",
      "Training epochs on cpu:  88%|███████▉ | 176/200 [00:58<00:07,  3.14epoch/s, loss=0.165, prev_loss=0.209]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 58.05batch/s]\u001b[A\n",
      "Training epochs on cpu:  88%|███████▉ | 177/200 [00:58<00:07,  3.06epoch/s, loss=0.199, prev_loss=0.165]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 72.22batch/s]\u001b[A\n",
      "Training epochs on cpu:  89%|████████▉ | 178/200 [00:58<00:06,  3.19epoch/s, loss=0.18, prev_loss=0.199]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 62.13batch/s]\u001b[A\n",
      "Training epochs on cpu:  90%|████████▉ | 179/200 [00:59<00:06,  3.20epoch/s, loss=0.214, prev_loss=0.18]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 61.03batch/s]\u001b[A\n",
      "Training epochs on cpu:  90%|████████ | 180/200 [00:59<00:06,  3.11epoch/s, loss=0.268, prev_loss=0.214]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 72.13batch/s]\u001b[A\n",
      "Training epochs on cpu:  90%|████████▏| 181/200 [00:59<00:05,  3.21epoch/s, loss=0.182, prev_loss=0.268]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 58.14batch/s]\u001b[A\n",
      "Training epochs on cpu:  91%|████████▏| 182/200 [01:00<00:05,  3.11epoch/s, loss=0.146, prev_loss=0.182]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 65.11batch/s]\u001b[A\n",
      "Training epochs on cpu:  92%|████████▏| 183/200 [01:00<00:05,  3.08epoch/s, loss=0.132, prev_loss=0.146]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 60.78batch/s]\u001b[A\n",
      "Training epochs on cpu:  92%|█████████▏| 184/200 [01:00<00:05,  3.02epoch/s, loss=0.16, prev_loss=0.132]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 62.16batch/s]\u001b[A\n",
      "Training epochs on cpu:  92%|█████████▎| 185/200 [01:01<00:05,  2.99epoch/s, loss=0.212, prev_loss=0.16]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 62.28batch/s]\u001b[A\n",
      "Training epochs on cpu:  93%|████████▎| 186/200 [01:01<00:04,  2.99epoch/s, loss=0.169, prev_loss=0.212]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 60.45batch/s]\u001b[A\n",
      "Training epochs on cpu:  94%|████████▍| 187/200 [01:01<00:04,  2.96epoch/s, loss=0.162, prev_loss=0.169]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 68.17batch/s]\u001b[A\n",
      "Training epochs on cpu:  94%|████████▍| 188/200 [01:02<00:03,  3.06epoch/s, loss=0.155, prev_loss=0.162]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 62.16batch/s]\u001b[A\n",
      "Training epochs on cpu:  94%|████████▌| 189/200 [01:02<00:03,  3.03epoch/s, loss=0.126, prev_loss=0.155]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.63batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 58.95batch/s]\u001b[A\n",
      "Training epochs on cpu:  95%|████████▌| 190/200 [01:02<00:03,  2.98epoch/s, loss=0.152, prev_loss=0.126]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 68.90batch/s]\u001b[A\n",
      "Training epochs on cpu:  96%|████████▌| 191/200 [01:03<00:02,  3.02epoch/s, loss=0.189, prev_loss=0.152]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 58.93batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 58.80batch/s]\u001b[A\n",
      "Training epochs on cpu:  96%|████████▋| 192/200 [01:03<00:02,  3.01epoch/s, loss=0.182, prev_loss=0.189]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 64.84batch/s]\u001b[A\n",
      "Training epochs on cpu:  96%|████████▋| 193/200 [01:03<00:02,  2.99epoch/s, loss=0.153, prev_loss=0.182]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.54batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 59.42batch/s]\u001b[A\n",
      "Training epochs on cpu:  97%|████████▋| 194/200 [01:04<00:02,  2.95epoch/s, loss=0.142, prev_loss=0.153]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 63.37batch/s]\u001b[A\n",
      "Training epochs on cpu:  98%|████████▊| 195/200 [01:04<00:01,  2.99epoch/s, loss=0.133, prev_loss=0.142]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 67.58batch/s]\u001b[A\n",
      "Training epochs on cpu:  98%|████████▊| 196/200 [01:04<00:01,  3.02epoch/s, loss=0.142, prev_loss=0.133]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 65.76batch/s]\u001b[A\n",
      "Training epochs on cpu:  98%|█████████▊| 197/200 [01:04<00:00,  3.10epoch/s, loss=0.13, prev_loss=0.142]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.44batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 59.44batch/s]\u001b[A\n",
      "Training epochs on cpu:  99%|█████████▉| 198/200 [01:05<00:00,  3.02epoch/s, loss=0.148, prev_loss=0.13]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 61.32batch/s]\u001b[A\n",
      "Training epochs on cpu: 100%|████████▉| 199/200 [01:05<00:00,  3.00epoch/s, loss=0.131, prev_loss=0.148]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 67.20batch/s]\u001b[A\n",
      "Training epochs on cpu: 100%|█████████| 200/200 [01:05<00:00,  3.03epoch/s, loss=0.127, prev_loss=0.131]\u001b[A\n",
      "INFO:pykeen.evaluation.evaluator:Currently automatic memory optimization only supports GPUs, but you're using a CPU. Therefore, the batch_size will be set to the default value.\n",
      "INFO:pykeen.evaluation.evaluator:No evaluation batch_size provided. Setting batch_size to '32'.\n",
      "Evaluating on cpu: 100%|████████████████████████████████████████████| 159/159 [00:00<00:00, 797triple/s]\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 0.21s seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005846267976101089\n"
     ]
    }
   ],
   "source": [
    "# but to get a better performing model, you want to set different things\n",
    "pipeline_result = pipeline(\n",
    "    random_seed=0,\n",
    "    model='ComplEx',\n",
    "    training=got_training,\n",
    "    testing=got_testing,\n",
    "    epochs=200,\n",
    "    dimensions=150,\n",
    "    optimizer='adam',\n",
    "    optimizer_kwargs={'lr':1e-3},\n",
    "    loss='pairwisehinge', \n",
    "    regularizer='LP', \n",
    "    regularizer_kwargs={'p':3, 'weight':1e-5}, \n",
    ")\n",
    "print(pipeline_result.get_metric('mrr'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the parameters:\n",
    "\n",
    "- dimensions : the dimensionality of the embedding space\n",
    "- negative_sampler : the negative samplic strategy, here set to default (not used in arguments).\n",
    "- batch_size : the number of batches in which the training set is split during the training loop. If you are having into low memory issues than settings this to a higher number may help.\n",
    "- epochs : the number of epochs to train the model for.\n",
    "- optimizer : the Adam optimizer, with a learning rate of $1e-3$ set via the <i>optimizer_kwarg</i>.\n",
    "- loss : pairwise loss, with a margin of $0.5$ set via the <i>loss_kwarg</i>.\n",
    "- regularizer :  regularization with $p=2$, i.e. $l_2$ regularization. $\\lambda$ = $1e-5$, set via the <i>regularizer_kwarg</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Negatives\n",
    "\n",
    "To ensure our model can be trained and evaluated correctly, we need to define a filter to ensure that no negative statements generated by the corruption procedure are actually positives. This is simply done by concatenating train and test sets. When negative triples are generated by the corruption strategy, we can check that they aren't actually true statements.\n",
    "\n",
    "With PyKEEN this is made very easy, and can simply be passed as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pykeen.pipeline.api:No random seed is specified. Setting to 522401844.\n",
      "WARNING:pykeen.utils:No cuda devices were available. The model runs on CPU\n",
      "INFO:pykeen.pipeline.api:Using device: None\n",
      "Training epochs on cpu:   0%|                                                | 0/200 [00:00<?, ?epoch/s]\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 81.40batch/s]\u001b[A\n",
      "Training epochs on cpu:   0%|              | 1/200 [00:00<00:51,  3.84epoch/s, loss=15.9, prev_loss=nan]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 73.07batch/s]\u001b[A\n",
      "Training epochs on cpu:   1%|▏            | 2/200 [00:00<00:55,  3.54epoch/s, loss=15.8, prev_loss=15.9]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 78.51batch/s]\u001b[A\n",
      "Training epochs on cpu:   2%|▏            | 3/200 [00:00<00:54,  3.59epoch/s, loss=14.4, prev_loss=15.8]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 77.07batch/s]\u001b[A\n",
      "Training epochs on cpu:   2%|▎            | 4/200 [00:01<00:54,  3.62epoch/s, loss=14.1, prev_loss=14.4]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 77.32batch/s]\u001b[A\n",
      "Training epochs on cpu:   2%|▎            | 5/200 [00:01<00:53,  3.64epoch/s, loss=13.4, prev_loss=14.1]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 78.28batch/s]\u001b[A\n",
      "Training epochs on cpu:   3%|▍            | 6/200 [00:01<00:52,  3.68epoch/s, loss=13.3, prev_loss=13.4]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 80.09batch/s]\u001b[A\n",
      "Training epochs on cpu:   4%|▍            | 7/200 [00:01<00:53,  3.59epoch/s, loss=12.2, prev_loss=13.3]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 68.19batch/s]\u001b[A\n",
      "Training epochs on cpu:   4%|▌            | 8/200 [00:02<00:53,  3.60epoch/s, loss=11.7, prev_loss=12.2]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 68.98batch/s]\u001b[A\n",
      "Training epochs on cpu:   4%|▋              | 9/200 [00:02<00:53,  3.60epoch/s, loss=11, prev_loss=11.7]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 83.00batch/s]\u001b[A\n",
      "Training epochs on cpu:   5%|▋             | 10/200 [00:02<00:52,  3.61epoch/s, loss=10.5, prev_loss=11]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 82.58batch/s]\u001b[A\n",
      "Training epochs on cpu:   6%|▋           | 11/200 [00:03<00:52,  3.58epoch/s, loss=10.4, prev_loss=10.5]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 75.17batch/s]\u001b[A\n",
      "Training epochs on cpu:   6%|▋           | 12/200 [00:03<00:53,  3.52epoch/s, loss=9.66, prev_loss=10.4]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 72.48batch/s]\u001b[A\n",
      "Training epochs on cpu:   6%|▊           | 13/200 [00:03<00:53,  3.48epoch/s, loss=9.35, prev_loss=9.66]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 74.57batch/s]\u001b[A\n",
      "Training epochs on cpu:   7%|▊           | 14/200 [00:03<00:52,  3.53epoch/s, loss=9.13, prev_loss=9.35]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 79.09batch/s]\u001b[A\n",
      "Training epochs on cpu:   8%|▉           | 15/200 [00:04<00:50,  3.63epoch/s, loss=8.75, prev_loss=9.13]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 70.44batch/s]\u001b[A\n",
      "Training epochs on cpu:   8%|▉           | 16/200 [00:04<00:52,  3.53epoch/s, loss=8.47, prev_loss=8.75]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 77.53batch/s]\u001b[A\n",
      "Training epochs on cpu:   8%|█           | 17/200 [00:04<00:50,  3.59epoch/s, loss=8.06, prev_loss=8.47]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 77.56batch/s]\u001b[A\n",
      "Training epochs on cpu:   9%|█           | 18/200 [00:05<00:49,  3.66epoch/s, loss=7.72, prev_loss=8.06]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 77.89batch/s]\u001b[A\n",
      "Training epochs on cpu:  10%|█▏          | 19/200 [00:05<00:50,  3.60epoch/s, loss=7.86, prev_loss=7.72]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 82.21batch/s]\u001b[A\n",
      "Training epochs on cpu:  10%|█▏          | 20/200 [00:05<00:48,  3.68epoch/s, loss=7.07, prev_loss=7.86]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 74.89batch/s]\u001b[A\n",
      "Training epochs on cpu:  10%|█▎           | 21/200 [00:05<00:49,  3.59epoch/s, loss=6.8, prev_loss=7.07]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 72.76batch/s]\u001b[A\n",
      "Training epochs on cpu:  11%|█▍           | 22/200 [00:06<00:49,  3.58epoch/s, loss=6.75, prev_loss=6.8]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 69.01batch/s]\u001b[A\n",
      "Training epochs on cpu:  12%|█▍           | 23/200 [00:06<00:50,  3.48epoch/s, loss=6.3, prev_loss=6.75]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 81.49batch/s]\u001b[A\n",
      "Training epochs on cpu:  12%|█▌           | 24/200 [00:06<00:50,  3.49epoch/s, loss=6.12, prev_loss=6.3]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 75.36batch/s]\u001b[A\n",
      "Training epochs on cpu:  12%|█▌          | 25/200 [00:06<00:49,  3.55epoch/s, loss=5.59, prev_loss=6.12]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 75.63batch/s]\u001b[A\n",
      "Training epochs on cpu:  13%|█▋           | 26/200 [00:07<00:49,  3.52epoch/s, loss=5.7, prev_loss=5.59]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 75.89batch/s]\u001b[A\n",
      "Training epochs on cpu:  14%|█▊           | 27/200 [00:07<00:48,  3.59epoch/s, loss=5.23, prev_loss=5.7]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 69.56batch/s]\u001b[A\n",
      "Training epochs on cpu:  14%|█▋          | 28/200 [00:07<00:48,  3.52epoch/s, loss=4.97, prev_loss=5.23]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 81.61batch/s]\u001b[A\n",
      "Training epochs on cpu:  14%|█▋          | 29/200 [00:08<00:47,  3.62epoch/s, loss=4.99, prev_loss=4.97]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 81.52batch/s]\u001b[A\n",
      "Training epochs on cpu:  15%|█▉           | 30/200 [00:08<00:46,  3.62epoch/s, loss=4.8, prev_loss=4.99]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 82.58batch/s]\u001b[A\n",
      "Training epochs on cpu:  16%|██           | 31/200 [00:08<00:46,  3.61epoch/s, loss=4.67, prev_loss=4.8]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 82.18batch/s]\u001b[A\n",
      "Training epochs on cpu:  16%|█▉          | 32/200 [00:08<00:45,  3.71epoch/s, loss=4.24, prev_loss=4.67]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 76.77batch/s]\u001b[A\n",
      "Training epochs on cpu:  16%|█▉          | 33/200 [00:09<00:44,  3.75epoch/s, loss=3.99, prev_loss=4.24]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 79.77batch/s]\u001b[A\n",
      "Training epochs on cpu:  17%|██          | 34/200 [00:09<00:44,  3.76epoch/s, loss=3.98, prev_loss=3.99]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 68.51batch/s]\u001b[A\n",
      "Training epochs on cpu:  18%|██          | 35/200 [00:09<00:45,  3.64epoch/s, loss=4.28, prev_loss=3.98]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 69.85batch/s]\u001b[A\n",
      "Training epochs on cpu:  18%|██▏         | 36/200 [00:10<00:46,  3.55epoch/s, loss=3.62, prev_loss=4.28]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 74.60batch/s]\u001b[A\n",
      "Training epochs on cpu:  18%|██▏         | 37/200 [00:10<00:46,  3.52epoch/s, loss=3.71, prev_loss=3.62]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 70.51batch/s]\u001b[A\n",
      "Training epochs on cpu:  19%|██▎         | 38/200 [00:10<00:45,  3.57epoch/s, loss=3.78, prev_loss=3.71]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 74.84batch/s]\u001b[A\n",
      "Training epochs on cpu:  20%|██▎         | 39/200 [00:10<00:45,  3.53epoch/s, loss=3.35, prev_loss=3.78]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 73.22batch/s]\u001b[A\n",
      "Training epochs on cpu:  20%|██▍         | 40/200 [00:11<00:45,  3.49epoch/s, loss=3.07, prev_loss=3.35]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 82.77batch/s]\u001b[A\n",
      "Training epochs on cpu:  20%|██▍         | 41/200 [00:11<00:44,  3.61epoch/s, loss=3.03, prev_loss=3.07]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 71.91batch/s]\u001b[A\n",
      "Training epochs on cpu:  21%|██▌         | 42/200 [00:11<00:44,  3.55epoch/s, loss=2.99, prev_loss=3.03]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 69.54batch/s]\u001b[A\n",
      "Training epochs on cpu:  22%|██▊          | 43/200 [00:12<00:45,  3.47epoch/s, loss=3.1, prev_loss=2.99]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 76.30batch/s]\u001b[A\n",
      "Training epochs on cpu:  22%|███▌            | 44/200 [00:12<00:45,  3.40epoch/s, loss=3, prev_loss=3.1]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 63.90batch/s]\u001b[A\n",
      "Training epochs on cpu:  22%|███▍           | 45/200 [00:12<00:47,  3.25epoch/s, loss=2.66, prev_loss=3]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 66.47batch/s]\u001b[A\n",
      "Training epochs on cpu:  23%|██▊         | 46/200 [00:12<00:47,  3.27epoch/s, loss=2.63, prev_loss=2.66]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 52.45batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 53.27batch/s]\u001b[A\n",
      "Training epochs on cpu:  24%|██▊         | 47/200 [00:13<00:50,  3.05epoch/s, loss=2.59, prev_loss=2.63]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 64.62batch/s]\u001b[A\n",
      "Training epochs on cpu:  24%|██▉         | 48/200 [00:13<00:50,  3.00epoch/s, loss=2.58, prev_loss=2.59]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 64.91batch/s]\u001b[A\n",
      "Training epochs on cpu:  24%|██▉         | 49/200 [00:14<00:50,  2.98epoch/s, loss=2.44, prev_loss=2.58]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 58.99batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 58.50batch/s]\u001b[A\n",
      "Training epochs on cpu:  25%|███         | 50/200 [00:14<00:51,  2.93epoch/s, loss=2.25, prev_loss=2.44]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 58.94batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 58.40batch/s]\u001b[A\n",
      "Training epochs on cpu:  26%|███         | 51/200 [00:14<00:51,  2.88epoch/s, loss=2.19, prev_loss=2.25]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.74batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 58.35batch/s]\u001b[A\n",
      "Training epochs on cpu:  26%|███         | 52/200 [00:15<00:51,  2.86epoch/s, loss=1.96, prev_loss=2.19]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 56.71batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 56.56batch/s]\u001b[A\n",
      "Training epochs on cpu:  26%|███▏        | 53/200 [00:15<00:51,  2.83epoch/s, loss=2.17, prev_loss=1.96]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 55.92batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 56.22batch/s]\u001b[A\n",
      "Training epochs on cpu:  27%|███▏        | 54/200 [00:15<00:51,  2.82epoch/s, loss=2.16, prev_loss=2.17]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 65.70batch/s]\u001b[A\n",
      "Training epochs on cpu:  28%|███▎        | 55/200 [00:16<00:50,  2.85epoch/s, loss=1.88, prev_loss=2.16]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 56.88batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 57.00batch/s]\u001b[A\n",
      "Training epochs on cpu:  28%|███▎        | 56/200 [00:16<00:50,  2.84epoch/s, loss=1.79, prev_loss=1.88]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 61.16batch/s]\u001b[A\n",
      "Training epochs on cpu:  28%|███▍        | 57/200 [00:16<00:50,  2.85epoch/s, loss=1.83, prev_loss=1.79]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 60.19batch/s]\u001b[A\n",
      "Training epochs on cpu:  29%|███▍        | 58/200 [00:17<00:48,  2.95epoch/s, loss=1.87, prev_loss=1.83]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 57.87batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 57.89batch/s]\u001b[A\n",
      "Training epochs on cpu:  30%|███▌        | 59/200 [00:17<00:47,  2.96epoch/s, loss=1.74, prev_loss=1.87]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 55.64batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 55.71batch/s]\u001b[A\n",
      "Training epochs on cpu:  30%|███▌        | 60/200 [00:17<00:48,  2.88epoch/s, loss=1.74, prev_loss=1.74]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 66.13batch/s]\u001b[A\n",
      "Training epochs on cpu:  30%|███▋        | 61/200 [00:18<00:48,  2.89epoch/s, loss=1.55, prev_loss=1.74]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 64.02batch/s]\u001b[A\n",
      "Training epochs on cpu:  31%|███▋        | 62/200 [00:18<00:47,  2.92epoch/s, loss=1.61, prev_loss=1.55]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.37batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 57.28batch/s]\u001b[A\n",
      "Training epochs on cpu:  32%|███▊        | 63/200 [00:18<00:46,  2.94epoch/s, loss=1.57, prev_loss=1.61]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 64.34batch/s]\u001b[A\n",
      "Training epochs on cpu:  32%|███▊        | 64/200 [00:19<00:46,  2.92epoch/s, loss=1.52, prev_loss=1.57]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.90batch/s]\u001b[A\n",
      "Training epochs on cpu:  32%|███▉        | 65/200 [00:19<00:45,  2.94epoch/s, loss=1.53, prev_loss=1.52]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 54.49batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 56.24batch/s]\u001b[A\n",
      "Training epochs on cpu:  33%|███▉        | 66/200 [00:19<00:46,  2.88epoch/s, loss=1.41, prev_loss=1.53]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 63.63batch/s]\u001b[A\n",
      "Training epochs on cpu:  34%|████        | 67/200 [00:20<00:46,  2.88epoch/s, loss=1.31, prev_loss=1.41]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 55.63batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 55.68batch/s]\u001b[A\n",
      "Training epochs on cpu:  34%|████        | 68/200 [00:20<00:46,  2.83epoch/s, loss=1.34, prev_loss=1.31]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 56.95batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 58.29batch/s]\u001b[A\n",
      "Training epochs on cpu:  34%|████▏       | 69/200 [00:21<00:46,  2.81epoch/s, loss=1.29, prev_loss=1.34]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 61.35batch/s]\u001b[A\n",
      "Training epochs on cpu:  35%|████▏       | 70/200 [00:21<00:46,  2.80epoch/s, loss=1.36, prev_loss=1.29]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 55.80batch/s]\u001b[A\n",
      "Training epochs on cpu:  36%|████▎       | 71/200 [00:21<00:45,  2.81epoch/s, loss=1.29, prev_loss=1.36]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 57.55batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 57.82batch/s]\u001b[A\n",
      "Training epochs on cpu:  36%|████▎       | 72/200 [00:22<00:45,  2.81epoch/s, loss=1.22, prev_loss=1.29]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 62.31batch/s]\u001b[A\n",
      "Training epochs on cpu:  36%|████▋        | 73/200 [00:22<00:44,  2.83epoch/s, loss=1.2, prev_loss=1.22]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 55.71batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 54.80batch/s]\u001b[A\n",
      "Training epochs on cpu:  37%|████▊        | 74/200 [00:22<00:45,  2.79epoch/s, loss=1.18, prev_loss=1.2]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 63.63batch/s]\u001b[A\n",
      "Training epochs on cpu:  38%|████▌       | 75/200 [00:23<00:42,  2.91epoch/s, loss=1.14, prev_loss=1.18]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 57.77batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 56.65batch/s]\u001b[A\n",
      "Training epochs on cpu:  38%|████▌       | 76/200 [00:23<00:43,  2.86epoch/s, loss=1.11, prev_loss=1.14]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 58.23batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 56.78batch/s]\u001b[A\n",
      "Training epochs on cpu:  38%|████▌       | 77/200 [00:23<00:43,  2.84epoch/s, loss=1.01, prev_loss=1.11]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.55batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 59.40batch/s]\u001b[A\n",
      "Training epochs on cpu:  39%|████▎      | 78/200 [00:24<00:41,  2.91epoch/s, loss=0.936, prev_loss=1.01]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 56.05batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 55.88batch/s]\u001b[A\n",
      "Training epochs on cpu:  40%|███▉      | 79/200 [00:24<00:41,  2.93epoch/s, loss=0.903, prev_loss=0.936]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 65.51batch/s]\u001b[A\n",
      "Training epochs on cpu:  40%|████      | 80/200 [00:24<00:40,  2.93epoch/s, loss=0.765, prev_loss=0.903]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.44batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 58.29batch/s]\u001b[A\n",
      "Training epochs on cpu:  40%|████▍      | 81/200 [00:25<00:40,  2.97epoch/s, loss=1.03, prev_loss=0.765]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 56.72batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 56.28batch/s]\u001b[A\n",
      "Training epochs on cpu:  41%|████▉       | 82/200 [00:25<00:40,  2.90epoch/s, loss=1.01, prev_loss=1.03]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 61.65batch/s]\u001b[A\n",
      "Training epochs on cpu:  42%|████▌      | 83/200 [00:25<00:40,  2.89epoch/s, loss=0.935, prev_loss=1.01]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 58.51batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 58.32batch/s]\u001b[A\n",
      "Training epochs on cpu:  42%|████▏     | 84/200 [00:26<00:40,  2.87epoch/s, loss=0.923, prev_loss=0.935]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 64.24batch/s]\u001b[A\n",
      "Training epochs on cpu:  42%|████▎     | 85/200 [00:26<00:39,  2.89epoch/s, loss=0.925, prev_loss=0.923]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 57.96batch/s]\u001b[A\n",
      "Training epochs on cpu:  43%|████▎     | 86/200 [00:26<00:38,  2.96epoch/s, loss=0.854, prev_loss=0.925]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 57.19batch/s]\u001b[A\n",
      "Training epochs on cpu:  44%|████▎     | 87/200 [00:27<00:37,  2.98epoch/s, loss=0.723, prev_loss=0.854]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 61.46batch/s]\u001b[A\n",
      "Training epochs on cpu:  44%|████▍     | 88/200 [00:27<00:37,  2.97epoch/s, loss=0.836, prev_loss=0.723]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 60.25batch/s]\u001b[A\n",
      "Training epochs on cpu:  44%|████▍     | 89/200 [00:27<00:36,  3.02epoch/s, loss=0.776, prev_loss=0.836]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 64.10batch/s]\u001b[A\n",
      "Training epochs on cpu:  45%|████▌     | 90/200 [00:28<00:36,  3.02epoch/s, loss=0.764, prev_loss=0.776]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 65.75batch/s]\u001b[A\n",
      "Training epochs on cpu:  46%|████▌     | 91/200 [00:28<00:36,  3.00epoch/s, loss=0.852, prev_loss=0.764]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 64.66batch/s]\u001b[A\n",
      "Training epochs on cpu:  46%|████▌     | 92/200 [00:28<00:36,  2.99epoch/s, loss=0.674, prev_loss=0.852]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 64.98batch/s]\u001b[A\n",
      "Training epochs on cpu:  46%|█████      | 93/200 [00:29<00:35,  2.98epoch/s, loss=0.71, prev_loss=0.674]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 68.86batch/s]\u001b[A\n",
      "Training epochs on cpu:  47%|█████▏     | 94/200 [00:29<00:35,  2.98epoch/s, loss=0.769, prev_loss=0.71]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 63.13batch/s]\u001b[A\n",
      "Training epochs on cpu:  48%|█████▏     | 95/200 [00:29<00:35,  2.97epoch/s, loss=0.61, prev_loss=0.769]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 60.93batch/s]\u001b[A\n",
      "Training epochs on cpu:  48%|█████▎     | 96/200 [00:30<00:34,  3.04epoch/s, loss=0.731, prev_loss=0.61]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 64.41batch/s]\u001b[A\n",
      "Training epochs on cpu:  48%|████▊     | 97/200 [00:30<00:33,  3.10epoch/s, loss=0.656, prev_loss=0.731]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 63.43batch/s]\u001b[A\n",
      "Training epochs on cpu:  49%|████▉     | 98/200 [00:30<00:32,  3.09epoch/s, loss=0.718, prev_loss=0.656]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.07batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 58.98batch/s]\u001b[A\n",
      "Training epochs on cpu:  50%|████▉     | 99/200 [00:31<00:32,  3.08epoch/s, loss=0.562, prev_loss=0.718]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 61.22batch/s]\u001b[A\n",
      "Training epochs on cpu:  50%|████▌    | 100/200 [00:31<00:33,  3.00epoch/s, loss=0.651, prev_loss=0.562]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 57.21batch/s]\u001b[A\n",
      "Training epochs on cpu:  50%|█████     | 101/200 [00:31<00:33,  2.96epoch/s, loss=0.63, prev_loss=0.651]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 65.73batch/s]\u001b[A\n",
      "Training epochs on cpu:  51%|█████     | 102/200 [00:32<00:33,  2.96epoch/s, loss=0.659, prev_loss=0.63]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.74batch/s]\u001b[A\n",
      "Training epochs on cpu:  52%|█████▏    | 103/200 [00:32<00:31,  3.03epoch/s, loss=0.68, prev_loss=0.659]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 57.55batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 57.07batch/s]\u001b[A\n",
      "Training epochs on cpu:  52%|█████▏    | 104/200 [00:32<00:32,  2.96epoch/s, loss=0.553, prev_loss=0.68]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 55.91batch/s]\u001b[A\n",
      "Training epochs on cpu:  52%|████▋    | 105/200 [00:33<00:32,  2.94epoch/s, loss=0.606, prev_loss=0.553]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 58.30batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 58.70batch/s]\u001b[A\n",
      "Training epochs on cpu:  53%|████▊    | 106/200 [00:33<00:32,  2.91epoch/s, loss=0.529, prev_loss=0.606]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 63.41batch/s]\u001b[A\n",
      "Training epochs on cpu:  54%|████▊    | 107/200 [00:33<00:30,  3.00epoch/s, loss=0.461, prev_loss=0.529]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 57.37batch/s]\u001b[A\n",
      "Training epochs on cpu:  54%|████▊    | 108/200 [00:34<00:30,  3.04epoch/s, loss=0.609, prev_loss=0.461]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 56.92batch/s]\u001b[A\n",
      "Training epochs on cpu:  55%|████▉    | 109/200 [00:34<00:29,  3.06epoch/s, loss=0.567, prev_loss=0.609]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.07batch/s]\u001b[A\n",
      "Training epochs on cpu:  55%|████▉    | 110/200 [00:34<00:29,  3.01epoch/s, loss=0.579, prev_loss=0.567]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 56.34batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 55.92batch/s]\u001b[A\n",
      "Training epochs on cpu:  56%|████▉    | 111/200 [00:35<00:30,  2.92epoch/s, loss=0.461, prev_loss=0.579]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 64.44batch/s]\u001b[A\n",
      "Training epochs on cpu:  56%|█████    | 112/200 [00:35<00:30,  2.91epoch/s, loss=0.483, prev_loss=0.461]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.46batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 59.37batch/s]\u001b[A\n",
      "Training epochs on cpu:  56%|█████    | 113/200 [00:35<00:30,  2.90epoch/s, loss=0.488, prev_loss=0.483]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 57.76batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 57.67batch/s]\u001b[A\n",
      "Training epochs on cpu:  57%|█████▏   | 114/200 [00:36<00:29,  2.87epoch/s, loss=0.528, prev_loss=0.488]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 66.08batch/s]\u001b[A\n",
      "Training epochs on cpu:  57%|█████▏   | 115/200 [00:36<00:28,  2.95epoch/s, loss=0.463, prev_loss=0.528]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 58.88batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 59.21batch/s]\u001b[A\n",
      "Training epochs on cpu:  58%|█████▏   | 116/200 [00:36<00:28,  2.98epoch/s, loss=0.547, prev_loss=0.463]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 63.99batch/s]\u001b[A\n",
      "Training epochs on cpu:  58%|█████▎   | 117/200 [00:37<00:27,  3.06epoch/s, loss=0.409, prev_loss=0.547]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 55.73batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 56.45batch/s]\u001b[A\n",
      "Training epochs on cpu:  59%|█████▉    | 118/200 [00:37<00:27,  2.97epoch/s, loss=0.46, prev_loss=0.409]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 63.57batch/s]\u001b[A\n",
      "Training epochs on cpu:  60%|█████▉    | 119/200 [00:37<00:26,  3.06epoch/s, loss=0.424, prev_loss=0.46]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 61.34batch/s]\u001b[A\n",
      "Training epochs on cpu:  60%|█████▍   | 120/200 [00:38<00:26,  3.01epoch/s, loss=0.491, prev_loss=0.424]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 56.44batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 55.72batch/s]\u001b[A\n",
      "Training epochs on cpu:  60%|██████    | 121/200 [00:38<00:26,  2.99epoch/s, loss=0.48, prev_loss=0.491]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 58.66batch/s]\u001b[A\n",
      "Training epochs on cpu:  61%|██████    | 122/200 [00:38<00:26,  2.97epoch/s, loss=0.379, prev_loss=0.48]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.96batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 59.82batch/s]\u001b[A\n",
      "Training epochs on cpu:  62%|█████▌   | 123/200 [00:39<00:26,  2.94epoch/s, loss=0.465, prev_loss=0.379]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 66.88batch/s]\u001b[A\n",
      "Training epochs on cpu:  62%|█████▌   | 124/200 [00:39<00:25,  2.97epoch/s, loss=0.351, prev_loss=0.465]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 57.64batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 57.65batch/s]\u001b[A\n",
      "Training epochs on cpu:  62%|█████▋   | 125/200 [00:39<00:25,  2.93epoch/s, loss=0.431, prev_loss=0.351]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 58.04batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 58.02batch/s]\u001b[A\n",
      "Training epochs on cpu:  63%|█████▋   | 126/200 [00:40<00:24,  2.97epoch/s, loss=0.355, prev_loss=0.431]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 62.81batch/s]\u001b[A\n",
      "Training epochs on cpu:  64%|█████▋   | 127/200 [00:40<00:24,  3.01epoch/s, loss=0.325, prev_loss=0.355]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 62.24batch/s]\u001b[A\n",
      "Training epochs on cpu:  64%|█████▊   | 128/200 [00:40<00:24,  2.99epoch/s, loss=0.409, prev_loss=0.325]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 58.76batch/s]\u001b[A\n",
      "Training epochs on cpu:  64%|█████▊   | 129/200 [00:41<00:23,  2.98epoch/s, loss=0.335, prev_loss=0.409]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 58.31batch/s]\u001b[A\n",
      "Training epochs on cpu:  65%|█████▊   | 130/200 [00:41<00:23,  3.00epoch/s, loss=0.278, prev_loss=0.335]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 65.76batch/s]\u001b[A\n",
      "Training epochs on cpu:  66%|█████▉   | 131/200 [00:41<00:22,  3.03epoch/s, loss=0.359, prev_loss=0.278]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 62.16batch/s]\u001b[A\n",
      "Training epochs on cpu:  66%|█████▉   | 132/200 [00:42<00:22,  2.99epoch/s, loss=0.294, prev_loss=0.359]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 67.73batch/s]\u001b[A\n",
      "Training epochs on cpu:  66%|█████▉   | 133/200 [00:42<00:22,  3.04epoch/s, loss=0.373, prev_loss=0.294]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 66.32batch/s]\u001b[A\n",
      "Training epochs on cpu:  67%|██████   | 134/200 [00:42<00:21,  3.02epoch/s, loss=0.266, prev_loss=0.373]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 58.60batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 58.66batch/s]\u001b[A\n",
      "Training epochs on cpu:  68%|██████   | 135/200 [00:43<00:21,  2.96epoch/s, loss=0.281, prev_loss=0.266]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 63.19batch/s]\u001b[A\n",
      "Training epochs on cpu:  68%|██████   | 136/200 [00:43<00:21,  2.96epoch/s, loss=0.357, prev_loss=0.281]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 62.19batch/s]\u001b[A\n",
      "Training epochs on cpu:  68%|██████▏  | 137/200 [00:43<00:21,  2.97epoch/s, loss=0.287, prev_loss=0.357]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 61.85batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs on cpu:  69%|██████▏  | 138/200 [00:44<00:21,  2.95epoch/s, loss=0.312, prev_loss=0.287]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 57.21batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 57.83batch/s]\u001b[A\n",
      "Training epochs on cpu:  70%|██████▎  | 139/200 [00:44<00:20,  2.91epoch/s, loss=0.265, prev_loss=0.312]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 64.81batch/s]\u001b[A\n",
      "Training epochs on cpu:  70%|██████▎  | 140/200 [00:44<00:20,  2.91epoch/s, loss=0.348, prev_loss=0.265]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 66.96batch/s]\u001b[A\n",
      "Training epochs on cpu:  70%|██████▎  | 141/200 [00:45<00:19,  3.02epoch/s, loss=0.306, prev_loss=0.348]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 56.92batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 56.42batch/s]\u001b[A\n",
      "Training epochs on cpu:  71%|██████▍  | 142/200 [00:45<00:19,  2.94epoch/s, loss=0.316, prev_loss=0.306]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 57.46batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 58.81batch/s]\u001b[A\n",
      "Training epochs on cpu:  72%|██████▍  | 143/200 [00:45<00:19,  2.95epoch/s, loss=0.304, prev_loss=0.316]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 57.71batch/s]\u001b[A\n",
      "Training epochs on cpu:  72%|██████▍  | 144/200 [00:46<00:18,  2.96epoch/s, loss=0.335, prev_loss=0.304]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 66.16batch/s]\u001b[A\n",
      "Training epochs on cpu:  72%|██████▌  | 145/200 [00:46<00:18,  3.03epoch/s, loss=0.258, prev_loss=0.335]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 60.35batch/s]\u001b[A\n",
      "Training epochs on cpu:  73%|███████▎  | 146/200 [00:46<00:17,  3.04epoch/s, loss=0.25, prev_loss=0.258]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 63.39batch/s]\u001b[A\n",
      "Training epochs on cpu:  74%|███████▎  | 147/200 [00:47<00:16,  3.12epoch/s, loss=0.249, prev_loss=0.25]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 64.84batch/s]\u001b[A\n",
      "Training epochs on cpu:  74%|██████▋  | 148/200 [00:47<00:16,  3.11epoch/s, loss=0.262, prev_loss=0.249]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 56.29batch/s]\u001b[A\n",
      "Training epochs on cpu:  74%|██████▋  | 149/200 [00:47<00:16,  3.09epoch/s, loss=0.229, prev_loss=0.262]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 62.38batch/s]\u001b[A\n",
      "Training epochs on cpu:  75%|██████▊  | 150/200 [00:48<00:16,  3.02epoch/s, loss=0.243, prev_loss=0.229]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 58.61batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 58.71batch/s]\u001b[A\n",
      "Training epochs on cpu:  76%|██████▊  | 151/200 [00:48<00:16,  3.01epoch/s, loss=0.319, prev_loss=0.243]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 59.88batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 59.66batch/s]\u001b[A\n",
      "Training epochs on cpu:  76%|██████▊  | 152/200 [00:48<00:15,  3.02epoch/s, loss=0.167, prev_loss=0.319]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 58.26batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 58.40batch/s]\u001b[A\n",
      "Training epochs on cpu:  76%|██████▉  | 153/200 [00:49<00:15,  2.98epoch/s, loss=0.258, prev_loss=0.167]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 54.68batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 49.78batch/s]\u001b[A\n",
      "Training epochs on cpu:  77%|██████▉  | 154/200 [00:49<00:16,  2.78epoch/s, loss=0.287, prev_loss=0.258]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 38.39batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 36.82batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 36.02batch/s]\u001b[A\n",
      "Training epochs on cpu:  78%|██████▉  | 155/200 [00:50<00:18,  2.47epoch/s, loss=0.251, prev_loss=0.287]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 35.13batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 33.04batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 32.85batch/s]\u001b[A\n",
      "Training epochs on cpu:  78%|███████  | 156/200 [00:50<00:20,  2.18epoch/s, loss=0.255, prev_loss=0.251]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 32.58batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 31.90batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 31.11batch/s]\u001b[A\n",
      "Training epochs on cpu:  78%|███████  | 157/200 [00:51<00:21,  2.00epoch/s, loss=0.319, prev_loss=0.255]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 30.03batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 29.60batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|███████████████████████████████████▊   | 11/12 [00:00<00:00, 28.05batch/s]\u001b[A\n",
      "Training epochs on cpu:  79%|███████  | 158/200 [00:52<00:23,  1.81epoch/s, loss=0.185, prev_loss=0.319]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 33.12batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 29.48batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|███████████████████████████████████▊   | 11/12 [00:00<00:00, 28.17batch/s]\u001b[A\n",
      "Training epochs on cpu:  80%|███████▏ | 159/200 [00:52<00:23,  1.72epoch/s, loss=0.262, prev_loss=0.185]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  25%|██████████                              | 3/12 [00:00<00:00, 27.42batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 26.07batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 26.26batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 26.40batch/s]\u001b[A\n",
      "Training epochs on cpu:  80%|███████▏ | 160/200 [00:53<00:24,  1.61epoch/s, loss=0.253, prev_loss=0.262]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 30.29batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 29.24batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|███████████████████████████████████▊   | 11/12 [00:00<00:00, 29.14batch/s]\u001b[A\n",
      "Training epochs on cpu:  80%|███████▏ | 161/200 [00:54<00:24,  1.61epoch/s, loss=0.195, prev_loss=0.253]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  25%|██████████                              | 3/12 [00:00<00:00, 29.25batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 27.50batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 28.03batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 28.61batch/s]\u001b[A\n",
      "Training epochs on cpu:  81%|███████▎ | 162/200 [00:54<00:24,  1.57epoch/s, loss=0.208, prev_loss=0.195]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 30.01batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 29.28batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|███████████████████████████████████▊   | 11/12 [00:00<00:00, 29.04batch/s]\u001b[A\n",
      "Training epochs on cpu:  82%|███████▎ | 163/200 [00:55<00:23,  1.57epoch/s, loss=0.207, prev_loss=0.208]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 30.45batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 29.33batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|███████████████████████████████████▊   | 11/12 [00:00<00:00, 29.01batch/s]\u001b[A\n",
      "Training epochs on cpu:  82%|███████▍ | 164/200 [00:55<00:22,  1.57epoch/s, loss=0.216, prev_loss=0.207]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 30.28batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 29.59batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|███████████████████████████████████▊   | 11/12 [00:00<00:00, 28.63batch/s]\u001b[A\n",
      "Training epochs on cpu:  82%|███████▍ | 165/200 [00:56<00:22,  1.59epoch/s, loss=0.206, prev_loss=0.216]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  25%|██████████                              | 3/12 [00:00<00:00, 29.71batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 28.31batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 28.71batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 29.13batch/s]\u001b[A\n",
      "Training epochs on cpu:  83%|███████▍ | 166/200 [00:57<00:21,  1.58epoch/s, loss=0.194, prev_loss=0.206]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  25%|██████████                              | 3/12 [00:00<00:00, 26.17batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 24.48batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 24.62batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 24.93batch/s]\u001b[A\n",
      "Training epochs on cpu:  84%|███████▌ | 167/200 [00:57<00:21,  1.51epoch/s, loss=0.206, prev_loss=0.194]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 31.36batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 31.23batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 29.49batch/s]\u001b[A\n",
      "Training epochs on cpu:  84%|███████▌ | 168/200 [00:58<00:20,  1.53epoch/s, loss=0.188, prev_loss=0.206]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  25%|██████████                              | 3/12 [00:00<00:00, 29.80batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 27.63batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 27.29batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 27.85batch/s]\u001b[A\n",
      "Training epochs on cpu:  84%|█████████▎ | 169/200 [00:59<00:20,  1.51epoch/s, loss=0.2, prev_loss=0.188]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 30.29batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 30.39batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 31.24batch/s]\u001b[A\n",
      "Training epochs on cpu:  85%|█████████▎ | 170/200 [00:59<00:19,  1.54epoch/s, loss=0.193, prev_loss=0.2]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 30.36batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 29.64batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|███████████████████████████████████▊   | 11/12 [00:00<00:00, 28.53batch/s]\u001b[A\n",
      "Training epochs on cpu:  86%|███████▋ | 171/200 [01:00<00:18,  1.56epoch/s, loss=0.281, prev_loss=0.193]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 31.04batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 31.30batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 31.41batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs on cpu:  86%|███████▋ | 172/200 [01:01<00:17,  1.58epoch/s, loss=0.202, prev_loss=0.281]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 31.02batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 29.48batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|███████████████████████████████████▊   | 11/12 [00:00<00:00, 28.20batch/s]\u001b[A\n",
      "Training epochs on cpu:  86%|███████▊ | 173/200 [01:01<00:17,  1.58epoch/s, loss=0.204, prev_loss=0.202]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 32.62batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 32.24batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 31.22batch/s]\u001b[A\n",
      "Training epochs on cpu:  87%|███████▊ | 174/200 [01:02<00:16,  1.60epoch/s, loss=0.164, prev_loss=0.204]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 30.41batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 29.87batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 30.68batch/s]\u001b[A\n",
      "Training epochs on cpu:  88%|███████▉ | 175/200 [01:03<00:15,  1.60epoch/s, loss=0.179, prev_loss=0.164]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  25%|██████████                              | 3/12 [00:00<00:00, 28.22batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 27.57batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 30.37batch/s]\u001b[A\n",
      "Training epochs on cpu:  88%|███████▉ | 176/200 [01:03<00:14,  1.61epoch/s, loss=0.153, prev_loss=0.179]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  25%|██████████                              | 3/12 [00:00<00:00, 29.58batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 29.65batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 30.17batch/s]\u001b[A\n",
      "Training epochs on cpu:  88%|███████▉ | 177/200 [01:04<00:14,  1.63epoch/s, loss=0.124, prev_loss=0.153]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  25%|██████████                              | 3/12 [00:00<00:00, 29.65batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 31.79batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|███████████████████████████████████▊   | 11/12 [00:00<00:00, 30.62batch/s]\u001b[A\n",
      "Training epochs on cpu:  89%|████████ | 178/200 [01:04<00:13,  1.61epoch/s, loss=0.176, prev_loss=0.124]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  25%|██████████                              | 3/12 [00:00<00:00, 27.74batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 25.95batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 27.13batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 28.18batch/s]\u001b[A\n",
      "Training epochs on cpu:  90%|████████ | 179/200 [01:05<00:13,  1.58epoch/s, loss=0.141, prev_loss=0.176]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 31.56batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 31.83batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 31.72batch/s]\u001b[A\n",
      "Training epochs on cpu:  90%|████████ | 180/200 [01:06<00:12,  1.63epoch/s, loss=0.175, prev_loss=0.141]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  25%|██████████                              | 3/12 [00:00<00:00, 29.99batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|████████████████████                    | 6/12 [00:00<00:00, 29.63batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 29.52batch/s]\u001b[A\n",
      "Training epochs on cpu:  90%|████████▏| 181/200 [01:06<00:11,  1.62epoch/s, loss=0.171, prev_loss=0.175]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  25%|██████████                              | 3/12 [00:00<00:00, 29.33batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 31.32batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|███████████████████████████████████▊   | 11/12 [00:00<00:00, 29.91batch/s]\u001b[A\n",
      "Training epochs on cpu:  91%|████████▏| 182/200 [01:07<00:11,  1.62epoch/s, loss=0.157, prev_loss=0.171]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 34.08batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 32.50batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 32.92batch/s]\u001b[A\n",
      "Training epochs on cpu:  92%|████████▏| 183/200 [01:07<00:10,  1.65epoch/s, loss=0.151, prev_loss=0.157]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 31.73batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 33.81batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 32.97batch/s]\u001b[A\n",
      "Training epochs on cpu:  92%|████████▎| 184/200 [01:08<00:09,  1.68epoch/s, loss=0.201, prev_loss=0.151]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 35.09batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 34.43batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 32.90batch/s]\u001b[A\n",
      "Training epochs on cpu:  92%|████████▎| 185/200 [01:09<00:08,  1.69epoch/s, loss=0.164, prev_loss=0.201]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 31.70batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 32.26batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 32.12batch/s]\u001b[A\n",
      "Training epochs on cpu:  93%|████████▎| 186/200 [01:09<00:08,  1.69epoch/s, loss=0.147, prev_loss=0.164]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 36.57batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 34.76batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 32.91batch/s]\u001b[A\n",
      "Training epochs on cpu:  94%|████████▍| 187/200 [01:10<00:07,  1.70epoch/s, loss=0.126, prev_loss=0.147]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 30.41batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 31.34batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 33.02batch/s]\u001b[A\n",
      "Training epochs on cpu:  94%|████████▍| 188/200 [01:10<00:07,  1.71epoch/s, loss=0.114, prev_loss=0.126]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 32.81batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 32.34batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 32.74batch/s]\u001b[A\n",
      "Training epochs on cpu:  94%|████████▌| 189/200 [01:11<00:06,  1.71epoch/s, loss=0.171, prev_loss=0.114]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 32.99batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 32.31batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 32.79batch/s]\u001b[A\n",
      "Training epochs on cpu:  95%|████████▌| 190/200 [01:11<00:05,  1.71epoch/s, loss=0.173, prev_loss=0.171]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 36.09batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 35.38batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 33.92batch/s]\u001b[A\n",
      "Training epochs on cpu:  96%|████████▌| 191/200 [01:12<00:05,  1.73epoch/s, loss=0.138, prev_loss=0.173]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 35.44batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 34.33batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 33.19batch/s]\u001b[A\n",
      "Training epochs on cpu:  96%|████████▋| 192/200 [01:13<00:04,  1.74epoch/s, loss=0.146, prev_loss=0.138]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 34.29batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 33.54batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 33.25batch/s]\u001b[A\n",
      "Training epochs on cpu:  96%|████████▋| 193/200 [01:13<00:04,  1.74epoch/s, loss=0.138, prev_loss=0.146]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 33.79batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 32.91batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 31.35batch/s]\u001b[A\n",
      "Training epochs on cpu:  97%|█████████▋| 194/200 [01:14<00:03,  1.70epoch/s, loss=0.13, prev_loss=0.138]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 32.85batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 33.87batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 34.33batch/s]\u001b[A\n",
      "Training epochs on cpu:  98%|█████████▊| 195/200 [01:14<00:02,  1.72epoch/s, loss=0.152, prev_loss=0.13]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 33.82batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 33.38batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 32.68batch/s]\u001b[A\n",
      "Training epochs on cpu:  98%|████████▊| 196/200 [01:15<00:02,  1.70epoch/s, loss=0.102, prev_loss=0.152]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 33.09batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 32.10batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 34.10batch/s]\u001b[A\n",
      "Training epochs on cpu:  98%|████████▊| 197/200 [01:16<00:01,  1.70epoch/s, loss=0.149, prev_loss=0.102]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 30.32batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 30.49batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 32.69batch/s]\u001b[A\n",
      "Training epochs on cpu:  99%|████████▉| 198/200 [01:16<00:01,  1.70epoch/s, loss=0.201, prev_loss=0.149]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 34.61batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 33.57batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 33.52batch/s]\u001b[A\n",
      "Training epochs on cpu: 100%|████████▉| 199/200 [01:17<00:00,  1.72epoch/s, loss=0.143, prev_loss=0.201]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  33%|█████████████▎                          | 4/12 [00:00<00:00, 38.60batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 36.76batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|███████████████████████████████████████| 12/12 [00:00<00:00, 35.60batch/s]\u001b[A\n",
      "Training epochs on cpu: 100%|█████████| 200/200 [01:17<00:00,  2.57epoch/s, loss=0.132, prev_loss=0.143]\u001b[A\n",
      "INFO:pykeen.evaluation.evaluator:Currently automatic memory optimization only supports GPUs, but you're using a CPU. Therefore, the batch_size will be set to the default value.\n",
      "INFO:pykeen.evaluation.evaluator:No evaluation batch_size provided. Setting batch_size to '32'.\n",
      "Evaluating on cpu: 100%|████████████████████████████████████████████| 159/159 [00:00<00:00, 384triple/s]\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 0.44s seconds\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pipeline_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m pipeline_result \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComplEx\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m     training\u001b[38;5;241m=\u001b[39mgot_training,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     )\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpipeline_results\u001b[49m\u001b[38;5;241m.\u001b[39mget_metric(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmrr\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline_results' is not defined"
     ]
    }
   ],
   "source": [
    "pipeline_result = pipeline(\n",
    "    model='ComplEx',\n",
    "    training=got_training,\n",
    "    testing=got_testing,\n",
    "    epochs=200,\n",
    "    dimensions=150,\n",
    "    optimizer='adam',\n",
    "    optimizer_kwargs={'lr':1e-3},\n",
    "    loss='pairwisehinge', \n",
    "    regularizer='LP', \n",
    "    regularizer_kwargs={'p':3, 'weight':1e-5}, \n",
    "    \n",
    "    negative_sampler='basic',\n",
    "    negative_sampler_kwargs=dict(\n",
    "        filtered=True,\n",
    "    )\n",
    ")\n",
    "print(pipeline_results.get_metric('mrr'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save your learned model and also the results, we need to add checkpoints to the pipeline.\n",
    "By adding training kwargs to the pipeline, the model will be automatically saved. By default, it saves the model after every epoch (checkpoint_frequency=0). You can also set the directory to which the models are saved, but by default they will end up in ~/.data/pykeen/checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pykeen.pipeline.api:loaded random seed 242643974 from checkpoint.\n",
      "WARNING:pykeen.utils:No cuda devices were available. The model runs on CPU\n",
      "INFO:pykeen.pipeline.api:Using device: None\n",
      "INFO:pykeen.training.training_loop:=> loading checkpoint 'checkpoint_dir/got_complex_checkpoint.pt'\n",
      "INFO:pykeen.training.training_loop:=> loaded checkpoint 'checkpoint_dir/got_complex_checkpoint.pt' stopped after having finished epoch 200\n",
      "INFO:pykeen.stoppers.stopper:=> loading stopper summary dict from training loop checkpoint in 'checkpoint_dir/got_complex_checkpoint.pt'\n",
      "INFO:pykeen.stoppers.stopper:=> loaded stopper summary dictionary from checkpoint in 'checkpoint_dir/got_complex_checkpoint.pt'\n",
      "WARNING:pykeen.training.training_loop:the training loop was configured with a stopper but no stopper configuration was saved in the checkpoint\n",
      "Training epochs on cpu: 100%|██████████████████████████████████████████████| 200/200 [00:00<?, ?epoch/s]\n",
      "INFO:pykeen.evaluation.evaluator:Currently automatic memory optimization only supports GPUs, but you're using a CPU. Therefore, the batch_size will be set to the default value.\n",
      "INFO:pykeen.evaluation.evaluator:No evaluation batch_size provided. Setting batch_size to '32'.\n",
      "Evaluating on cpu: 100%|████████████████████████████████████████████| 159/159 [00:00<00:00, 870triple/s]\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 0.19s seconds\n"
     ]
    }
   ],
   "source": [
    "pipeline_result = pipeline(\n",
    "    model='ComplEx',\n",
    "    training=got_training,\n",
    "    testing=got_testing,\n",
    "    training_kwargs=dict(\n",
    "        num_epochs=200,\n",
    "        checkpoint_name='got_complex_checkpoint.pt',\n",
    "        checkpoint_directory='checkpoint_dir/',\n",
    "        checkpoint_frequency=20,\n",
    "    ),\n",
    "    dimensions=150,\n",
    "    optimizer='adam',\n",
    "    optimizer_kwargs={'lr':1e-3},\n",
    "    loss='pairwisehinge', \n",
    "    regularizer='LP', \n",
    "    regularizer_kwargs={'p':3, 'weight':1e-5}, \n",
    "    negative_sampler='basic',\n",
    "    negative_sampler_kwargs=dict(\n",
    "        filtered=True,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another way to save models, but for that we need to do the training and evaluating outside of the pipeline model. Below is an example of the above model training outside of the pipeline module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pykeen.models.base:No random seed is specified. This may lead to non-reproducible results.\n",
      "INFO:pykeen.training.training_loop:Currently automatic memory optimization only supports GPUs, but you're using a CPU. Therefore, the batch_size will be set to the default value '{batch_size}'\n",
      "Training epochs on cpu:   0%|                                                | 0/200 [00:00<?, ?epoch/s]\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 95.80batch/s]\u001b[A\n",
      "Training epochs on cpu:   0%|                | 1/200 [00:00<01:05,  3.05epoch/s, loss=11, prev_loss=nan]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 102.41batch/s]\u001b[A\n",
      "Training epochs on cpu:   1%|▏              | 2/200 [00:00<01:02,  3.16epoch/s, loss=10.7, prev_loss=11]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████████████████████████████████| 12/12 [00:00<00:00, 112.68batch/s]\u001b[A\n",
      "Training epochs on cpu:   2%|▏            | 3/200 [00:00<01:02,  3.13epoch/s, loss=10.2, prev_loss=10.7]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 108.66batch/s]\u001b[A\n",
      "Training epochs on cpu:   2%|▎            | 4/200 [00:01<01:01,  3.20epoch/s, loss=9.95, prev_loss=10.2]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 102.53batch/s]\u001b[A\n",
      "Training epochs on cpu:   2%|▎            | 5/200 [00:01<01:03,  3.09epoch/s, loss=9.69, prev_loss=9.95]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████████████████████████████████| 12/12 [00:00<00:00, 112.06batch/s]\u001b[A\n",
      "Training epochs on cpu:   3%|▍            | 6/200 [00:01<01:03,  3.06epoch/s, loss=9.62, prev_loss=9.69]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████████████████████████████████| 12/12 [00:00<00:00, 112.53batch/s]\u001b[A\n",
      "Training epochs on cpu:   4%|▍            | 7/200 [00:02<01:01,  3.14epoch/s, loss=9.03, prev_loss=9.62]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:   4%|▌            | 8/200 [00:02<00:59,  3.22epoch/s, loss=8.87, prev_loss=9.03]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████████████████████████████████| 12/12 [00:00<00:00, 119.32batch/s]\u001b[A\n",
      "Training epochs on cpu:   4%|▌            | 9/200 [00:02<01:00,  3.15epoch/s, loss=8.41, prev_loss=8.87]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 104.52batch/s]\u001b[A\n",
      "Training epochs on cpu:   5%|▌           | 10/200 [00:03<00:59,  3.17epoch/s, loss=8.24, prev_loss=8.41]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:   6%|▋           | 11/200 [00:03<00:58,  3.25epoch/s, loss=7.91, prev_loss=8.24]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████████████████████████████████| 12/12 [00:00<00:00, 116.46batch/s]\u001b[A\n",
      "Training epochs on cpu:   6%|▋           | 12/200 [00:03<00:57,  3.28epoch/s, loss=7.77, prev_loss=7.91]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 106.98batch/s]\u001b[A\n",
      "Training epochs on cpu:   6%|▊           | 13/200 [00:04<00:58,  3.19epoch/s, loss=7.63, prev_loss=7.77]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 109.56batch/s]\u001b[A\n",
      "Training epochs on cpu:   7%|▊           | 14/200 [00:04<00:58,  3.15epoch/s, loss=7.32, prev_loss=7.63]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 104.08batch/s]\u001b[A\n",
      "Training epochs on cpu:   8%|▉           | 15/200 [00:04<00:58,  3.15epoch/s, loss=7.05, prev_loss=7.32]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 107.32batch/s]\u001b[A\n",
      "Training epochs on cpu:   8%|▉           | 16/200 [00:05<00:59,  3.08epoch/s, loss=7.08, prev_loss=7.05]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:   8%|█           | 17/200 [00:05<00:59,  3.08epoch/s, loss=6.91, prev_loss=7.08]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 102.08batch/s]\u001b[A\n",
      "Training epochs on cpu:   9%|█           | 18/200 [00:05<01:00,  3.00epoch/s, loss=6.78, prev_loss=6.91]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  10%|█▏          | 19/200 [00:06<00:57,  3.14epoch/s, loss=6.76, prev_loss=6.78]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████████████████████████████████| 12/12 [00:00<00:00, 118.83batch/s]\u001b[A\n",
      "Training epochs on cpu:  10%|█▏          | 20/200 [00:06<00:57,  3.16epoch/s, loss=6.53, prev_loss=6.76]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  10%|█▎          | 21/200 [00:06<00:55,  3.23epoch/s, loss=6.36, prev_loss=6.53]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 98.90batch/s]\u001b[A\n",
      "Training epochs on cpu:  11%|█▍           | 22/200 [00:06<00:56,  3.13epoch/s, loss=6.4, prev_loss=6.36]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 102.34batch/s]\u001b[A\n",
      "Training epochs on cpu:  12%|█▍           | 23/200 [00:07<00:58,  3.04epoch/s, loss=5.88, prev_loss=6.4]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 106.77batch/s]\u001b[A\n",
      "Training epochs on cpu:  12%|█▍          | 24/200 [00:07<00:58,  3.01epoch/s, loss=5.84, prev_loss=5.88]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 101.78batch/s]\u001b[A\n",
      "Training epochs on cpu:  12%|█▋           | 25/200 [00:08<00:59,  2.96epoch/s, loss=5.8, prev_loss=5.84]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 102.93batch/s]\u001b[A\n",
      "Training epochs on cpu:  13%|█▋           | 26/200 [00:08<00:59,  2.94epoch/s, loss=5.66, prev_loss=5.8]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 103.15batch/s]\u001b[A\n",
      "Training epochs on cpu:  14%|█▌          | 27/200 [00:08<00:58,  2.98epoch/s, loss=5.59, prev_loss=5.66]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  14%|█▋          | 28/200 [00:09<00:57,  3.00epoch/s, loss=5.51, prev_loss=5.59]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  14%|█▋          | 29/200 [00:09<00:56,  3.03epoch/s, loss=5.46, prev_loss=5.51]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  15%|█▊          | 30/200 [00:09<00:54,  3.13epoch/s, loss=5.58, prev_loss=5.46]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 105.95batch/s]\u001b[A\n",
      "Training epochs on cpu:  16%|██           | 31/200 [00:09<00:55,  3.05epoch/s, loss=5.5, prev_loss=5.58]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████████████████████████████████| 12/12 [00:00<00:00, 114.89batch/s]\u001b[A\n",
      "Training epochs on cpu:  16%|██           | 32/200 [00:10<00:55,  3.00epoch/s, loss=5.32, prev_loss=5.5]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 95.88batch/s]\u001b[A\n",
      "Training epochs on cpu:  16%|██▏          | 33/200 [00:10<00:55,  3.03epoch/s, loss=5.5, prev_loss=5.32]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 107.39batch/s]\u001b[A\n",
      "Training epochs on cpu:  17%|██▏          | 34/200 [00:11<00:55,  3.01epoch/s, loss=5.06, prev_loss=5.5]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  18%|██          | 35/200 [00:11<00:54,  3.05epoch/s, loss=5.42, prev_loss=5.06]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 105.30batch/s]\u001b[A\n",
      "Training epochs on cpu:  18%|██▏         | 36/200 [00:11<00:54,  2.99epoch/s, loss=4.96, prev_loss=5.42]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 108.63batch/s]\u001b[A\n",
      "Training epochs on cpu:  18%|██▏         | 37/200 [00:11<00:53,  3.07epoch/s, loss=5.19, prev_loss=4.96]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  19%|██▎         | 38/200 [00:12<00:51,  3.14epoch/s, loss=4.96, prev_loss=5.19]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 100.51batch/s]\u001b[A\n",
      "Training epochs on cpu:  20%|██▌          | 39/200 [00:12<00:51,  3.11epoch/s, loss=5.1, prev_loss=4.96]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 106.36batch/s]\u001b[A\n",
      "Training epochs on cpu:  20%|██▌          | 40/200 [00:12<00:52,  3.04epoch/s, loss=4.96, prev_loss=5.1]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 107.84batch/s]\u001b[A\n",
      "Training epochs on cpu:  20%|██▍         | 41/200 [00:13<00:51,  3.06epoch/s, loss=4.97, prev_loss=4.96]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  21%|██▌         | 42/200 [00:13<00:49,  3.18epoch/s, loss=5.18, prev_loss=4.97]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  22%|██▌         | 43/200 [00:13<00:49,  3.20epoch/s, loss=4.68, prev_loss=5.18]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  22%|██▋         | 44/200 [00:14<00:49,  3.16epoch/s, loss=4.75, prev_loss=4.68]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 107.24batch/s]\u001b[A\n",
      "Training epochs on cpu:  22%|██▋         | 45/200 [00:14<00:50,  3.08epoch/s, loss=5.03, prev_loss=4.75]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 106.43batch/s]\u001b[A\n",
      "Training epochs on cpu:  23%|██▊         | 46/200 [00:14<00:50,  3.04epoch/s, loss=5.23, prev_loss=5.03]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 104.48batch/s]\u001b[A\n",
      "Training epochs on cpu:  24%|██▊         | 47/200 [00:15<00:50,  3.01epoch/s, loss=4.84, prev_loss=5.23]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 105.30batch/s]\u001b[A\n",
      "Training epochs on cpu:  24%|██▉         | 48/200 [00:15<00:50,  3.03epoch/s, loss=4.79, prev_loss=4.84]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  24%|██▉         | 49/200 [00:15<00:49,  3.02epoch/s, loss=4.62, prev_loss=4.79]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 105.10batch/s]\u001b[A\n",
      "Training epochs on cpu:  25%|███         | 50/200 [00:16<00:49,  3.00epoch/s, loss=4.89, prev_loss=4.62]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 108.25batch/s]\u001b[A\n",
      "Training epochs on cpu:  26%|███         | 51/200 [00:16<00:49,  3.02epoch/s, loss=4.77, prev_loss=4.89]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  26%|███         | 52/200 [00:16<00:47,  3.14epoch/s, loss=4.62, prev_loss=4.77]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  26%|███▏        | 53/200 [00:17<00:47,  3.12epoch/s, loss=4.66, prev_loss=4.62]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████████████████████████████████| 12/12 [00:00<00:00, 112.37batch/s]\u001b[A\n",
      "Training epochs on cpu:  27%|███▏        | 54/200 [00:17<00:47,  3.06epoch/s, loss=4.57, prev_loss=4.66]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 109.74batch/s]\u001b[A\n",
      "Training epochs on cpu:  28%|███▎        | 55/200 [00:17<00:46,  3.11epoch/s, loss=4.75, prev_loss=4.57]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  28%|███▎        | 56/200 [00:18<00:45,  3.19epoch/s, loss=4.66, prev_loss=4.75]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  28%|███▍        | 57/200 [00:18<00:44,  3.25epoch/s, loss=4.47, prev_loss=4.66]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 102.13batch/s]\u001b[A\n",
      "Training epochs on cpu:  29%|███▍        | 58/200 [00:18<00:44,  3.20epoch/s, loss=5.03, prev_loss=4.47]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  30%|███▌        | 59/200 [00:19<00:43,  3.26epoch/s, loss=4.67, prev_loss=5.03]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 108.44batch/s]\u001b[A\n",
      "Training epochs on cpu:  30%|███▌        | 60/200 [00:19<00:43,  3.21epoch/s, loss=4.82, prev_loss=4.67]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 106.59batch/s]\u001b[A\n",
      "Training epochs on cpu:  30%|███▋        | 61/200 [00:19<00:44,  3.11epoch/s, loss=4.44, prev_loss=4.82]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  31%|███▋        | 62/200 [00:19<00:43,  3.15epoch/s, loss=4.58, prev_loss=4.44]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 108.04batch/s]\u001b[A\n",
      "Training epochs on cpu:  32%|███▊        | 63/200 [00:20<00:44,  3.08epoch/s, loss=4.42, prev_loss=4.58]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 94.72batch/s]\u001b[A\n",
      "Training epochs on cpu:  32%|███▊        | 64/200 [00:20<00:45,  2.98epoch/s, loss=4.68, prev_loss=4.42]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 105.70batch/s]\u001b[A\n",
      "Training epochs on cpu:  32%|███▉        | 65/200 [00:21<00:45,  2.96epoch/s, loss=4.52, prev_loss=4.68]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  33%|███▉        | 66/200 [00:21<00:43,  3.06epoch/s, loss=4.54, prev_loss=4.52]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 99.05batch/s]\u001b[A\n",
      "Training epochs on cpu:  34%|████        | 67/200 [00:21<00:44,  2.98epoch/s, loss=4.63, prev_loss=4.54]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 91.67batch/s]\u001b[A\n",
      "Training epochs on cpu:  34%|████        | 68/200 [00:22<00:44,  2.94epoch/s, loss=4.49, prev_loss=4.63]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 103.14batch/s]\u001b[A\n",
      "Training epochs on cpu:  34%|████▏       | 69/200 [00:22<00:44,  2.92epoch/s, loss=4.56, prev_loss=4.49]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 109.30batch/s]\u001b[A\n",
      "Training epochs on cpu:  35%|████▏       | 70/200 [00:22<00:44,  2.92epoch/s, loss=4.46, prev_loss=4.56]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 105.75batch/s]\u001b[A\n",
      "Training epochs on cpu:  36%|████▎       | 71/200 [00:23<00:44,  2.91epoch/s, loss=4.24, prev_loss=4.46]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  36%|████▎       | 72/200 [00:23<00:42,  3.03epoch/s, loss=4.31, prev_loss=4.24]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 107.81batch/s]\u001b[A\n",
      "Training epochs on cpu:  36%|████▍       | 73/200 [00:23<00:41,  3.09epoch/s, loss=4.22, prev_loss=4.31]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  37%|████▍       | 74/200 [00:23<00:39,  3.19epoch/s, loss=4.47, prev_loss=4.22]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 108.90batch/s]\u001b[A\n",
      "Training epochs on cpu:  38%|████▌       | 75/200 [00:24<00:40,  3.11epoch/s, loss=4.34, prev_loss=4.47]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 101.33batch/s]\u001b[A\n",
      "Training epochs on cpu:  38%|████▌       | 76/200 [00:24<00:40,  3.03epoch/s, loss=4.31, prev_loss=4.34]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 104.19batch/s]\u001b[A\n",
      "Training epochs on cpu:  38%|████▌       | 77/200 [00:25<00:41,  2.98epoch/s, loss=4.19, prev_loss=4.31]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 99.45batch/s]\u001b[A\n",
      "Training epochs on cpu:  39%|████▋       | 78/200 [00:25<00:41,  2.95epoch/s, loss=4.37, prev_loss=4.19]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 97.94batch/s]\u001b[A\n",
      "Training epochs on cpu:  40%|████▋       | 79/200 [00:25<00:41,  2.92epoch/s, loss=4.34, prev_loss=4.37]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████████████████████████████████| 12/12 [00:00<00:00, 118.73batch/s]\u001b[A\n",
      "Training epochs on cpu:  40%|█████▏       | 80/200 [00:26<00:40,  2.93epoch/s, loss=4.5, prev_loss=4.34]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 103.60batch/s]\u001b[A\n",
      "Training epochs on cpu:  40%|█████▎       | 81/200 [00:26<00:40,  2.91epoch/s, loss=4.23, prev_loss=4.5]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 105.39batch/s]\u001b[A\n",
      "Training epochs on cpu:  41%|████▉       | 82/200 [00:26<00:40,  2.92epoch/s, loss=4.37, prev_loss=4.23]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 96.87batch/s]\u001b[A\n",
      "Training epochs on cpu:  42%|████▉       | 83/200 [00:27<00:39,  2.97epoch/s, loss=4.31, prev_loss=4.37]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 108.96batch/s]\u001b[A\n",
      "Training epochs on cpu:  42%|█████       | 84/200 [00:27<00:38,  3.05epoch/s, loss=4.15, prev_loss=4.31]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 105.70batch/s]\u001b[A\n",
      "Training epochs on cpu:  42%|█████       | 85/200 [00:27<00:37,  3.05epoch/s, loss=4.34, prev_loss=4.15]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████████████████████████████████| 12/12 [00:00<00:00, 112.27batch/s]\u001b[A\n",
      "Training epochs on cpu:  43%|█████▏      | 86/200 [00:27<00:36,  3.11epoch/s, loss=4.11, prev_loss=4.34]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 104.59batch/s]\u001b[A\n",
      "Training epochs on cpu:  44%|█████▏      | 87/200 [00:28<00:37,  3.05epoch/s, loss=4.36, prev_loss=4.11]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 103.84batch/s]\u001b[A\n",
      "Training epochs on cpu:  44%|█████▎      | 88/200 [00:28<00:36,  3.03epoch/s, loss=4.18, prev_loss=4.36]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 99.04batch/s]\u001b[A\n",
      "Training epochs on cpu:  44%|█████▎      | 89/200 [00:29<00:38,  2.87epoch/s, loss=4.11, prev_loss=4.18]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 90.35batch/s]\u001b[A\n",
      "Training epochs on cpu:  45%|█████▍      | 90/200 [00:29<00:39,  2.80epoch/s, loss=4.28, prev_loss=4.11]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 106.13batch/s]\u001b[A\n",
      "Training epochs on cpu:  46%|█████▍      | 91/200 [00:29<00:39,  2.77epoch/s, loss=4.07, prev_loss=4.28]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 91.41batch/s]\u001b[A\n",
      "Training epochs on cpu:  46%|█████▌      | 92/200 [00:30<00:39,  2.71epoch/s, loss=4.09, prev_loss=4.07]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 91.94batch/s]\u001b[A\n",
      "Training epochs on cpu:  46%|█████▌      | 93/200 [00:30<00:39,  2.69epoch/s, loss=4.29, prev_loss=4.09]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 88.08batch/s]\u001b[A\n",
      "Training epochs on cpu:  47%|█████▋      | 94/200 [00:30<00:40,  2.63epoch/s, loss=4.07, prev_loss=4.29]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 77.67batch/s]\u001b[A\n",
      "Training epochs on cpu:  48%|█████▋      | 95/200 [00:31<00:41,  2.55epoch/s, loss=4.06, prev_loss=4.07]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 94.48batch/s]\u001b[A\n",
      "Training epochs on cpu:  48%|█████▊      | 96/200 [00:31<00:39,  2.64epoch/s, loss=3.91, prev_loss=4.06]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 91.89batch/s]\u001b[A\n",
      "Training epochs on cpu:  48%|█████▊      | 97/200 [00:32<00:39,  2.62epoch/s, loss=3.98, prev_loss=3.91]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 86.05batch/s]\u001b[A\n",
      "Training epochs on cpu:  49%|█████▉      | 98/200 [00:32<00:37,  2.70epoch/s, loss=4.03, prev_loss=3.98]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 103.69batch/s]\u001b[A\n",
      "Training epochs on cpu:  50%|█████▉      | 99/200 [00:32<00:36,  2.78epoch/s, loss=4.07, prev_loss=4.03]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 101.30batch/s]\u001b[A\n",
      "Training epochs on cpu:  50%|█████▌     | 100/200 [00:33<00:36,  2.78epoch/s, loss=3.98, prev_loss=4.07]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 89.55batch/s]\u001b[A\n",
      "Training epochs on cpu:  50%|█████▌     | 101/200 [00:33<00:35,  2.82epoch/s, loss=3.91, prev_loss=3.98]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 87.94batch/s]\u001b[A\n",
      "Training epochs on cpu:  51%|█████▌     | 102/200 [00:33<00:35,  2.80epoch/s, loss=3.85, prev_loss=3.91]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 92.62batch/s]\u001b[A\n",
      "Training epochs on cpu:  52%|█████▋     | 103/200 [00:34<00:35,  2.72epoch/s, loss=4.05, prev_loss=3.85]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 83.56batch/s]\u001b[A\n",
      "Training epochs on cpu:  52%|█████▋     | 104/200 [00:34<00:35,  2.67epoch/s, loss=3.82, prev_loss=4.05]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 94.21batch/s]\u001b[A\n",
      "Training epochs on cpu:  52%|█████▊     | 105/200 [00:35<00:34,  2.75epoch/s, loss=4.06, prev_loss=3.82]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 90.88batch/s]\u001b[A\n",
      "Training epochs on cpu:  53%|█████▊     | 106/200 [00:35<00:34,  2.71epoch/s, loss=3.72, prev_loss=4.06]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 93.27batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs on cpu:  54%|█████▉     | 107/200 [00:35<00:34,  2.70epoch/s, loss=3.76, prev_loss=3.72]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 95.18batch/s]\u001b[A\n",
      "Training epochs on cpu:  54%|█████▉     | 108/200 [00:36<00:34,  2.70epoch/s, loss=3.82, prev_loss=3.76]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|███████████████████████████████████▊   | 11/12 [00:00<00:00, 98.60batch/s]\u001b[A\n",
      "Training epochs on cpu:  55%|█████▉     | 109/200 [00:36<00:33,  2.70epoch/s, loss=4.11, prev_loss=3.82]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 86.79batch/s]\u001b[A\n",
      "Training epochs on cpu:  55%|██████     | 110/200 [00:36<00:32,  2.74epoch/s, loss=3.88, prev_loss=4.11]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 92.65batch/s]\u001b[A\n",
      "Training epochs on cpu:  56%|██████     | 111/200 [00:37<00:32,  2.77epoch/s, loss=3.83, prev_loss=3.88]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 85.28batch/s]\u001b[A\n",
      "Training epochs on cpu:  56%|██████▏    | 112/200 [00:37<00:32,  2.68epoch/s, loss=3.89, prev_loss=3.83]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 92.32batch/s]\u001b[A\n",
      "Training epochs on cpu:  56%|██████▏    | 113/200 [00:37<00:32,  2.65epoch/s, loss=3.94, prev_loss=3.89]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 85.32batch/s]\u001b[A\n",
      "Training epochs on cpu:  57%|██████▎    | 114/200 [00:38<00:32,  2.67epoch/s, loss=3.92, prev_loss=3.94]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 77.88batch/s]\u001b[A\n",
      "Training epochs on cpu:  57%|██████▎    | 115/200 [00:38<00:32,  2.62epoch/s, loss=3.89, prev_loss=3.92]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  58%|███████████████████████▎                | 7/12 [00:00<00:00, 68.44batch/s]\u001b[A\n",
      "Training epochs on cpu:  58%|██████▍    | 116/200 [00:39<00:32,  2.58epoch/s, loss=4.02, prev_loss=3.89]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 76.22batch/s]\u001b[A\n",
      "Training epochs on cpu:  58%|██████▍    | 117/200 [00:39<00:32,  2.52epoch/s, loss=3.61, prev_loss=4.02]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 77.87batch/s]\u001b[A\n",
      "Training epochs on cpu:  59%|██████▍    | 118/200 [00:39<00:33,  2.48epoch/s, loss=3.69, prev_loss=3.61]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 83.44batch/s]\u001b[A\n",
      "Training epochs on cpu:  60%|██████▌    | 119/200 [00:40<00:32,  2.47epoch/s, loss=3.79, prev_loss=3.69]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 80.47batch/s]\u001b[A\n",
      "Training epochs on cpu:  60%|███████▏    | 120/200 [00:40<00:32,  2.43epoch/s, loss=3.8, prev_loss=3.79]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 75.69batch/s]\u001b[A\n",
      "Training epochs on cpu:  60%|███████▎    | 121/200 [00:41<00:31,  2.48epoch/s, loss=3.73, prev_loss=3.8]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 80.32batch/s]\u001b[A\n",
      "Training epochs on cpu:  61%|██████▋    | 122/200 [00:41<00:31,  2.45epoch/s, loss=3.65, prev_loss=3.73]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 82.74batch/s]\u001b[A\n",
      "Training epochs on cpu:  62%|██████▊    | 123/200 [00:42<00:30,  2.49epoch/s, loss=3.64, prev_loss=3.65]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 81.45batch/s]\u001b[A\n",
      "Training epochs on cpu:  62%|██████▊    | 124/200 [00:42<00:30,  2.48epoch/s, loss=3.74, prev_loss=3.64]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 80.56batch/s]\u001b[A\n",
      "Training epochs on cpu:  62%|██████▉    | 125/200 [00:42<00:29,  2.54epoch/s, loss=3.69, prev_loss=3.74]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 90.35batch/s]\u001b[A\n",
      "Training epochs on cpu:  63%|██████▉    | 126/200 [00:43<00:28,  2.59epoch/s, loss=3.78, prev_loss=3.69]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 78.42batch/s]\u001b[A\n",
      "Training epochs on cpu:  64%|██████▉    | 127/200 [00:43<00:28,  2.55epoch/s, loss=3.67, prev_loss=3.78]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 78.68batch/s]\u001b[A\n",
      "Training epochs on cpu:  64%|███████▋    | 128/200 [00:43<00:27,  2.58epoch/s, loss=3.6, prev_loss=3.67]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 83.16batch/s]\u001b[A\n",
      "Training epochs on cpu:  64%|███████▋    | 129/200 [00:44<00:28,  2.50epoch/s, loss=3.72, prev_loss=3.6]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 72.73batch/s]\u001b[A\n",
      "Training epochs on cpu:  65%|███████▊    | 130/200 [00:44<00:28,  2.48epoch/s, loss=3.6, prev_loss=3.72]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 75.15batch/s]\u001b[A\n",
      "Training epochs on cpu:  66%|███████▊    | 131/200 [00:45<00:28,  2.46epoch/s, loss=3.65, prev_loss=3.6]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 85.50batch/s]\u001b[A\n",
      "Training epochs on cpu:  66%|███████▎   | 132/200 [00:45<00:27,  2.49epoch/s, loss=3.46, prev_loss=3.65]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 84.33batch/s]\u001b[A\n",
      "Training epochs on cpu:  66%|███████▉    | 133/200 [00:46<00:27,  2.48epoch/s, loss=3.4, prev_loss=3.46]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  67%|██████████████████████████▋             | 8/12 [00:00<00:00, 78.21batch/s]\u001b[A\n",
      "Training epochs on cpu:  67%|████████    | 134/200 [00:46<00:27,  2.44epoch/s, loss=3.57, prev_loss=3.4]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 80.93batch/s]\u001b[A\n",
      "Training epochs on cpu:  68%|███████▍   | 135/200 [00:46<00:26,  2.44epoch/s, loss=3.38, prev_loss=3.57]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 107.24batch/s]\u001b[A\n",
      "Training epochs on cpu:  68%|███████▍   | 136/200 [00:47<00:25,  2.52epoch/s, loss=3.54, prev_loss=3.38]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 95.75batch/s]\u001b[A\n",
      "Training epochs on cpu:  68%|███████▌   | 137/200 [00:47<00:23,  2.65epoch/s, loss=3.46, prev_loss=3.54]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 91.45batch/s]\u001b[A\n",
      "Training epochs on cpu:  69%|███████▌   | 138/200 [00:47<00:22,  2.72epoch/s, loss=3.48, prev_loss=3.46]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 91.53batch/s]\u001b[A\n",
      "Training epochs on cpu:  70%|███████▋   | 139/200 [00:48<00:22,  2.71epoch/s, loss=3.36, prev_loss=3.48]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 93.45batch/s]\u001b[A\n",
      "Training epochs on cpu:  70%|███████▋   | 140/200 [00:48<00:22,  2.70epoch/s, loss=3.51, prev_loss=3.36]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 98.36batch/s]\u001b[A\n",
      "Training epochs on cpu:  70%|███████▊   | 141/200 [00:48<00:21,  2.78epoch/s, loss=3.38, prev_loss=3.51]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 91.21batch/s]\u001b[A\n",
      "Training epochs on cpu:  71%|███████▊   | 142/200 [00:49<00:20,  2.83epoch/s, loss=3.49, prev_loss=3.38]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 89.51batch/s]\u001b[A\n",
      "Training epochs on cpu:  72%|███████▊   | 143/200 [00:49<00:19,  2.85epoch/s, loss=3.45, prev_loss=3.49]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 84.82batch/s]\u001b[A\n",
      "Training epochs on cpu:  72%|███████▉   | 144/200 [00:50<00:20,  2.76epoch/s, loss=3.26, prev_loss=3.45]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 94.99batch/s]\u001b[A\n",
      "Training epochs on cpu:  72%|████████▋   | 145/200 [00:50<00:19,  2.76epoch/s, loss=3.5, prev_loss=3.26]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 92.78batch/s]\u001b[A\n",
      "Training epochs on cpu:  73%|████████▊   | 146/200 [00:50<00:19,  2.73epoch/s, loss=3.41, prev_loss=3.5]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 90.14batch/s]\u001b[A\n",
      "Training epochs on cpu:  74%|████████▊   | 147/200 [00:51<00:19,  2.71epoch/s, loss=3.4, prev_loss=3.41]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 103.64batch/s]\u001b[A\n",
      "Training epochs on cpu:  74%|████████▉   | 148/200 [00:51<00:18,  2.75epoch/s, loss=3.38, prev_loss=3.4]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 93.80batch/s]\u001b[A\n",
      "Training epochs on cpu:  74%|████████▏  | 149/200 [00:51<00:18,  2.81epoch/s, loss=3.52, prev_loss=3.38]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 104.57batch/s]\u001b[A\n",
      "Training epochs on cpu:  75%|████████▎  | 150/200 [00:52<00:17,  2.89epoch/s, loss=3.18, prev_loss=3.52]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 86.45batch/s]\u001b[A\n",
      "Training epochs on cpu:  76%|████████▎  | 151/200 [00:52<00:17,  2.79epoch/s, loss=3.21, prev_loss=3.18]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 89.78batch/s]\u001b[A\n",
      "Training epochs on cpu:  76%|████████▎  | 152/200 [00:52<00:16,  2.83epoch/s, loss=3.18, prev_loss=3.21]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 92.39batch/s]\u001b[A\n",
      "Training epochs on cpu:  76%|████████▍  | 153/200 [00:53<00:16,  2.87epoch/s, loss=3.38, prev_loss=3.18]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 91.50batch/s]\u001b[A\n",
      "Training epochs on cpu:  77%|████████▍  | 154/200 [00:53<00:16,  2.80epoch/s, loss=3.28, prev_loss=3.38]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 94.70batch/s]\u001b[A\n",
      "Training epochs on cpu:  78%|████████▌  | 155/200 [00:53<00:16,  2.75epoch/s, loss=3.26, prev_loss=3.28]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 91.67batch/s]\u001b[A\n",
      "Training epochs on cpu:  78%|████████▌  | 156/200 [00:54<00:16,  2.75epoch/s, loss=3.32, prev_loss=3.26]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 86.75batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs on cpu:  78%|█████████▍  | 157/200 [00:54<00:15,  2.73epoch/s, loss=3.1, prev_loss=3.32]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████████████████████████████████| 12/12 [00:00<00:00, 112.54batch/s]\u001b[A\n",
      "Training epochs on cpu:  79%|█████████▍  | 158/200 [00:55<00:14,  2.84epoch/s, loss=3.27, prev_loss=3.1]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 108.05batch/s]\u001b[A\n",
      "Training epochs on cpu:  80%|████████▋  | 159/200 [00:55<00:14,  2.84epoch/s, loss=3.13, prev_loss=3.27]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 94.14batch/s]\u001b[A\n",
      "Training epochs on cpu:  80%|████████▊  | 160/200 [00:55<00:14,  2.78epoch/s, loss=3.31, prev_loss=3.13]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 93.71batch/s]\u001b[A\n",
      "Training epochs on cpu:  80%|████████▊  | 161/200 [00:56<00:14,  2.75epoch/s, loss=3.15, prev_loss=3.31]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 89.72batch/s]\u001b[A\n",
      "Training epochs on cpu:  81%|████████▉  | 162/200 [00:56<00:14,  2.71epoch/s, loss=3.22, prev_loss=3.15]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 93.94batch/s]\u001b[A\n",
      "Training epochs on cpu:  82%|████████▉  | 163/200 [00:56<00:13,  2.73epoch/s, loss=3.43, prev_loss=3.22]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████████████████████████████████| 12/12 [00:00<00:00, 115.07batch/s]\u001b[A\n",
      "Training epochs on cpu:  82%|█████████  | 164/200 [00:57<00:12,  2.80epoch/s, loss=3.18, prev_loss=3.43]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 107.66batch/s]\u001b[A\n",
      "Training epochs on cpu:  82%|█████████  | 165/200 [00:57<00:12,  2.78epoch/s, loss=3.19, prev_loss=3.18]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████████████████████████████████| 12/12 [00:00<00:00, 114.02batch/s]\u001b[A\n",
      "Training epochs on cpu:  83%|█████████▏ | 166/200 [00:57<00:11,  2.89epoch/s, loss=3.24, prev_loss=3.19]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 85.17batch/s]\u001b[A\n",
      "Training epochs on cpu:  84%|█████████▏ | 167/200 [00:58<00:11,  2.79epoch/s, loss=3.06, prev_loss=3.24]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 99.06batch/s]\u001b[A\n",
      "Training epochs on cpu:  84%|█████████▏ | 168/200 [00:58<00:11,  2.75epoch/s, loss=3.03, prev_loss=3.06]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 93.94batch/s]\u001b[A\n",
      "Training epochs on cpu:  84%|█████████▎ | 169/200 [00:59<00:11,  2.79epoch/s, loss=3.17, prev_loss=3.03]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 89.52batch/s]\u001b[A\n",
      "Training epochs on cpu:  85%|█████████▎ | 170/200 [00:59<00:10,  2.74epoch/s, loss=3.09, prev_loss=3.17]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 93.89batch/s]\u001b[A\n",
      "Training epochs on cpu:  86%|█████████▍ | 171/200 [00:59<00:10,  2.74epoch/s, loss=3.07, prev_loss=3.09]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 96.78batch/s]\u001b[A\n",
      "Training epochs on cpu:  86%|██████████▎ | 172/200 [01:00<00:09,  2.82epoch/s, loss=3.2, prev_loss=3.07]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 91.43batch/s]\u001b[A\n",
      "Training epochs on cpu:  86%|██████████▍ | 173/200 [01:00<00:09,  2.77epoch/s, loss=3.01, prev_loss=3.2]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 94.92batch/s]\u001b[A\n",
      "Training epochs on cpu:  87%|█████████▌ | 174/200 [01:00<00:09,  2.73epoch/s, loss=2.91, prev_loss=3.01]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 81.91batch/s]\u001b[A\n",
      "Training epochs on cpu:  88%|█████████▋ | 175/200 [01:01<00:09,  2.69epoch/s, loss=3.12, prev_loss=2.91]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████████████████████████████████| 12/12 [00:00<00:00, 112.50batch/s]\u001b[A\n",
      "Training epochs on cpu:  88%|█████████▋ | 176/200 [01:01<00:08,  2.72epoch/s, loss=2.96, prev_loss=3.12]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████████████████████████████████| 12/12 [00:00<00:00, 110.44batch/s]\u001b[A\n",
      "Training epochs on cpu:  88%|█████████▋ | 177/200 [01:01<00:08,  2.74epoch/s, loss=3.06, prev_loss=2.96]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 92.38batch/s]\u001b[A\n",
      "Training epochs on cpu:  89%|█████████▊ | 178/200 [01:02<00:08,  2.72epoch/s, loss=3.01, prev_loss=3.06]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████████████████████████████████| 12/12 [00:00<00:00, 111.05batch/s]\u001b[A\n",
      "Training epochs on cpu:  90%|█████████▊ | 179/200 [01:02<00:07,  2.83epoch/s, loss=3.04, prev_loss=3.01]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 109.24batch/s]\u001b[A\n",
      "Training epochs on cpu:  90%|█████████▉ | 180/200 [01:02<00:06,  2.91epoch/s, loss=2.94, prev_loss=3.04]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 86.71batch/s]\u001b[A\n",
      "Training epochs on cpu:  90%|█████████▉ | 181/200 [01:03<00:06,  2.81epoch/s, loss=2.93, prev_loss=2.94]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 95.34batch/s]\u001b[A\n",
      "Training epochs on cpu:  91%|██████████ | 182/200 [01:03<00:06,  2.83epoch/s, loss=2.98, prev_loss=2.93]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 91.67batch/s]\u001b[A\n",
      "Training epochs on cpu:  92%|██████████ | 183/200 [01:04<00:05,  2.85epoch/s, loss=2.94, prev_loss=2.98]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 89.00batch/s]\u001b[A\n",
      "Training epochs on cpu:  92%|██████████ | 184/200 [01:04<00:05,  2.76epoch/s, loss=2.84, prev_loss=2.94]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 107.68batch/s]\u001b[A\n",
      "Training epochs on cpu:  92%|██████████▏| 185/200 [01:04<00:05,  2.76epoch/s, loss=2.88, prev_loss=2.84]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 94.05batch/s]\u001b[A\n",
      "Training epochs on cpu:  93%|██████████▏| 186/200 [01:05<00:05,  2.76epoch/s, loss=2.89, prev_loss=2.88]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 94.75batch/s]\u001b[A\n",
      "Training epochs on cpu:  94%|██████████▎| 187/200 [01:05<00:04,  2.74epoch/s, loss=2.82, prev_loss=2.89]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████████████████████████████████| 12/12 [00:00<00:00, 112.66batch/s]\u001b[A\n",
      "Training epochs on cpu:  94%|██████████▎| 188/200 [01:05<00:04,  2.78epoch/s, loss=2.79, prev_loss=2.82]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 85.97batch/s]\u001b[A\n",
      "Training epochs on cpu:  94%|███████████▎| 189/200 [01:06<00:03,  2.76epoch/s, loss=2.8, prev_loss=2.79]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  92%|██████████████████████████████████▊   | 11/12 [00:00<00:00, 107.67batch/s]\u001b[A\n",
      "Training epochs on cpu:  95%|███████████▍| 190/200 [01:06<00:03,  2.76epoch/s, loss=2.88, prev_loss=2.8]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 92.83batch/s]\u001b[A\n",
      "Training epochs on cpu:  96%|██████████▌| 191/200 [01:06<00:03,  2.80epoch/s, loss=2.82, prev_loss=2.88]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  75%|██████████████████████████████          | 9/12 [00:00<00:00, 88.09batch/s]\u001b[A\n",
      "Training epochs on cpu:  96%|██████████▌| 192/200 [01:07<00:02,  2.75epoch/s, loss=2.72, prev_loss=2.82]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 93.78batch/s]\u001b[A\n",
      "Training epochs on cpu:  96%|██████████▌| 193/200 [01:07<00:02,  2.72epoch/s, loss=2.83, prev_loss=2.72]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 91.97batch/s]\u001b[A\n",
      "Training epochs on cpu:  97%|██████████▋| 194/200 [01:08<00:02,  2.70epoch/s, loss=2.83, prev_loss=2.83]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 98.64batch/s]\u001b[A\n",
      "Training epochs on cpu:  98%|██████████▋| 195/200 [01:08<00:01,  2.79epoch/s, loss=2.78, prev_loss=2.83]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 91.66batch/s]\u001b[A\n",
      "Training epochs on cpu:  98%|██████████▊| 196/200 [01:08<00:01,  2.71epoch/s, loss=2.71, prev_loss=2.78]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████████████████████████████████| 12/12 [00:00<00:00, 116.29batch/s]\u001b[A\n",
      "Training epochs on cpu:  98%|██████████▊| 197/200 [01:09<00:01,  2.83epoch/s, loss=2.85, prev_loss=2.71]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 96.03batch/s]\u001b[A\n",
      "Training epochs on cpu:  99%|██████████▉| 198/200 [01:09<00:00,  2.88epoch/s, loss=2.67, prev_loss=2.85]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 90.68batch/s]\u001b[A\n",
      "Training epochs on cpu: 100%|██████████▉| 199/200 [01:09<00:00,  2.80epoch/s, loss=2.81, prev_loss=2.67]\u001b[A\n",
      "Training batches on cpu:   0%|                                                | 0/12 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  83%|████████████████████████████████▌      | 10/12 [00:00<00:00, 95.40batch/s]\u001b[A\n",
      "Training epochs on cpu: 100%|███████████| 200/200 [01:10<00:00,  2.85epoch/s, loss=2.69, prev_loss=2.81]\u001b[A\n",
      "INFO:pykeen.evaluation.evaluator:Currently automatic memory optimization only supports GPUs, but you're using a CPU. Therefore, the batch_size will be set to the default value.\n",
      "WARNING:pykeen.utils:The filtered setting was enabled, but there were no `additional_filter_triples`\n",
      "given. This means you probably forgot to pass (at least) the training triples. Try:\n",
      "\n",
      "    additional_filter_triples=[dataset.training.mapped_triples]\n",
      "\n",
      "Or if you want to use the Bordes et al. (2013) approach to filtering, do:\n",
      "\n",
      "    additional_filter_triples=[\n",
      "        dataset.training.mapped_triples,\n",
      "        dataset.validation.mapped_triples,\n",
      "    ]\n",
      "\n",
      "INFO:pykeen.evaluation.evaluator:No evaluation batch_size provided. Setting batch_size to '32'.\n",
      "Evaluating on cpu: 100%|████████████████████████████████████████████| 159/159 [00:00<00:00, 728triple/s]\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 0.22s seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pykeen.evaluation.rank_based_evaluator.RankBasedMetricResults object at 0x7f9af90aed60>\n"
     ]
    }
   ],
   "source": [
    "from pykeen.models import ComplEx\n",
    "model = ComplEx(triples_factory=got_training)\n",
    "\n",
    "from pykeen.optimizers import Adam\n",
    "optimizer = Adam(params=model.get_grad_params())\n",
    "\n",
    "# from pykeen.regularizers import LP\n",
    "# regularizer = LP(p=3,weight=1e-5)\n",
    "\n",
    "from pykeen.training import SLCWATrainingLoop\n",
    "training_loop = SLCWATrainingLoop(model=model,\n",
    "                                  triples_factory=got_training,\n",
    "                                  optimizer=optimizer)\n",
    "\n",
    "#training\n",
    "_ = training_loop.train(triples_factory=got_training,\n",
    "                    num_epochs=200)\n",
    "\n",
    "#evaluating\n",
    "from pykeen.evaluation import RankBasedEvaluator\n",
    "evaluator = RankBasedEvaluator()\n",
    "mapped_triples = got_testing.mapped_triples\n",
    "\n",
    "results = evaluator.evaluate(\n",
    "            model=model,\n",
    "            mapped_triples=mapped_triples,\n",
    "            )\n",
    "\n",
    "print(results.get_metric('mrr'))\n",
    "\n",
    "#save results, this works also with the pipeline results, as the results object \n",
    "#returned by the evaluator is the same as the one returned from the pipeline\n",
    "results.save_to_directory('got_complex')\n",
    "\n",
    "import torch\n",
    "torch.save(model,'trained_model.pkl')\n",
    "\n",
    "#to load the model use the following command\n",
    "# my_pykeen_model = torch.load('trained_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Try changing the parameters of your training process. See if you obtain a better model in terms of average loss. Save it as ./data/best_model.pkl. Which parameters work best for the dataset? \n",
    "\n",
    "Now use the training and test set you created in Exercise 2. Which loss you obtain, and for which parameters? \n",
    "\n",
    "Remember to save each model locally with a different name, so you can find them back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pykeen.models.predict:_predict is an expensive operation, involving 42025000 score evaluations.\n",
      "WARNING:pykeen.models.predict:Not providing k to score_all_triples entails huge memory requirements for reasonably-sized knowledge graphs.\n",
      "scoring: 100%|█████████████████████████████████████████████████| 20.5k/20.5k [00:08<00:00, 2.32kbatch/s]\n"
     ]
    }
   ],
   "source": [
    "from ampligraph.evaluation import evaluate_performance\n",
    "\n",
    "ranks = evaluate_performance(X_test, \n",
    "                             model=model, \n",
    "                             filter_triples=positives_filter,   # Corruption strategy filter defined above \n",
    "                             use_default_protocol=True, # corrupt subj and obj separately while evaluating\n",
    "                             verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScorePack(result=tensor([[1245,    2, 1080],\n",
      "        [1872,    4, 1440],\n",
      "        [1372,    1, 1123],\n",
      "        ...,\n",
      "        [1509,    2, 1421],\n",
      "        [1529,    9,  696],\n",
      "        [ 773,    8, 2021]]), scores=tensor([ 157.3499,  146.2950,  144.8752,  ..., -143.8732, -148.4619,\n",
      "        -151.1418]))\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diving in into the arguments of this function:\n",
    "- <b>X</b> : the data to evaluate on. We use our test set X to evaluate.\n",
    "- <b>model</b> : the model we previously trained.\n",
    "- <b>filter_triples</b> : this filters out the false negatives generated by the corruption strategy.\n",
    "- <b>use_default_protocol</b> : specifies whether to use the default corruption protocol. If True, then subj and obj are corrupted separately during evaluation.\n",
    "- <b>verbose</b> : this gives some nice log statements.\n",
    "\n",
    "The ranks returned by the <i>evaluate_performance</i> function indicate the rank at which the test set triple was found when performing link prediction using the model.\n",
    "\n",
    "For example, given the triple:\n",
    "\n",
    "<House Stark of Winterfell, IN_REGION, The North>\n",
    "\n",
    "The model returns a rank of 7. This tells us that while it is not the highest likelihood true statement (which would be given a rank 1), it is still pretty likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 1] ['House Branch' 'IN_REGION' 'The North']\n"
     ]
    }
   ],
   "source": [
    "print(ranks[0],X_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now get some evaluation metrics for our model, they were already computed during evaluation time as part of the pipeline, and print them out.\n",
    "\n",
    "We are going to use the following evaluation metrics:\n",
    "- <i>mrr</i> (mean reciprocal rank) : this function computes the mean of the reciprocal of elements of a vector of rankings ranks\n",
    "- <i>hits_at_n</i> : this function computes how many elements of a vector of rankings ranks make it to the $top_n$ positions.\n",
    "\n",
    "NB : The choice of which _N_ makes more sense depends on the application and the size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_result.get_metric('hits_at_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR: 0.0041\n",
      "\n",
      "Hits@10: 0.000000\n",
      "Interpretation: on average, the model guessed the correct subject or object 0.0% of the time when considering the top-10 better ranked triples.\n",
      "\n",
      "Hits@3: 0.000000\n",
      "Interpretation: on average, the model guessed the correct subject or object 0.0% of the time when considering the top-3 better ranked triples.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from ampligraph.evaluation import mr_score, mrr_score, hits_at_n_score\n",
    "\n",
    "mrr = pipeline_result.get_metric('mrr')\n",
    "print(\"MRR: %.4f\" % (mrr))\n",
    "print()\n",
    "\n",
    "hits_10 = pipeline_result.get_metric('hits_at_10')\n",
    "print(\"Hits@10: %.6f\" % (hits_10))\n",
    "print(\"Interpretation: on average, the model guessed the correct subject or object %.1f%% of the time when considering the top-10 better ranked triples.\\n\" % (hits_10*100))\n",
    "\n",
    "hits_3 = pipeline_result.get_metric('hits_at_3')\n",
    "print(\"Hits@3: %.6f\" % (hits_3))\n",
    "print(\"Interpretation: on average, the model guessed the correct subject or object %.1f%% of the time when considering the top-3 better ranked triples.\\n\" % (hits_3*100))\n",
    "\n",
    "# hits_1 = hits_at_n_score(ranks, n=1)\n",
    "# print(\"Hits@1: %.2f\" % (hits_1))\n",
    "# print(\"Interpretation: on average, the model guessed the correct subject or object %.1f%% of the time when considering the top-1 better ranked triples.\\n\" % (hits_1*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Evaluate the models you created before (different set sizes, different parameters). Summarise your results in a table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Link Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link prediction allows to infer missing links in a graph. This has many real-world use cases, such as predicting connections between people in a social network, interactions between proteins in a biological network, and music recommendation based on prior user taste.\n",
    "\n",
    "In our case, we are going to see which of the following candidate statements is more likely to be true. Note that the candidate statements below are made up, i.e. they are not in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:00<00:00, 68.51it/s]\n"
     ]
    }
   ],
   "source": [
    "X_unseen = np.array([\n",
    "    ['Jorah Mormont', 'SPOUSE', 'Daenerys Targaryen'],\n",
    "    ['Tyrion Lannister', 'SPOUSE', 'Missandei'],\n",
    "    [\"King's Landing\", 'SEAT_OF', 'House Lannister of Casterly Rock'],\n",
    "    ['Sansa Stark', 'SPOUSE', 'Petyr Baelish'],\n",
    "    ['Daenerys Targaryen', 'SPOUSE', 'Jon Snow'],\n",
    "    ['Daenerys Targaryen', 'SPOUSE', 'Craster'],\n",
    "    ['House Stark of Winterfell', 'IN_REGION', 'The North'],\n",
    "    ['House Stark of Winterfell', 'IN_REGION', 'Dorne'],\n",
    "    ['House Tyrell of Highgarden', 'IN_REGION', 'Beyond the Wall'],\n",
    "    ['Brandon Stark', 'ALLIED_WITH', 'House Stark of Winterfell'],\n",
    "    ['Brandon Stark', 'ALLIED_WITH', 'House Lannister of Casterly Rock'],    \n",
    "    ['Rhaegar Targaryen', 'PARENT_OF', 'Jon Snow'],\n",
    "    ['House Hutcheson', 'SWORN_TO', 'House Tyrell of Highgarden'],\n",
    "    ['Daenerys Targaryen', 'ALLIED_WITH', 'House Stark of Winterfell'],\n",
    "    ['Daenerys Targaryen', 'ALLIED_WITH', 'House Lannister of Casterly Rock'],\n",
    "    ['Jaime Lannister', 'PARENT_OF', 'Myrcella Baratheon'],\n",
    "    ['Robert I Baratheon', 'PARENT_OF', 'Myrcella Baratheon'],\n",
    "    ['Cersei Lannister', 'PARENT_OF', 'Myrcella Baratheon'],\n",
    "    ['Cersei Lannister', 'PARENT_OF', 'Brandon Stark'],\n",
    "    [\"Tywin Lannister\", 'PARENT_OF', 'Jaime Lannister'],\n",
    "    [\"Missandei\", 'SPOUSE', 'Grey Worm'],\n",
    "    [\"Brienne of Tarth\", 'SPOUSE', 'Jaime Lannister']\n",
    "])\n",
    "\n",
    "unseen_filter = np.array(list({tuple(i) for i in np.vstack((positives_filter, X_unseen))}))\n",
    "\n",
    "ranks_unseen = evaluate_performance(\n",
    "    X_unseen, \n",
    "    model=model, \n",
    "    filter_triples=unseen_filter,   # Corruption strategy filter defined above \n",
    "    corrupt_side = 's+o',\n",
    "    use_default_protocol=False, # corrupt subj and obj separately while evaluating\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Brandon Stark ALLIED_WITH House Lannister of C...</td>\n",
       "      <td>3998</td>\n",
       "      <td>-2.925931</td>\n",
       "      <td>0.050886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Cersei Lannister PARENT_OF Brandon Stark</td>\n",
       "      <td>4079</td>\n",
       "      <td>-2.160664</td>\n",
       "      <td>0.103339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jorah Mormont SPOUSE Daenerys Targaryen</td>\n",
       "      <td>3319</td>\n",
       "      <td>-0.859091</td>\n",
       "      <td>0.297529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tyrion Lannister SPOUSE Missandei</td>\n",
       "      <td>2977</td>\n",
       "      <td>-0.542991</td>\n",
       "      <td>0.367492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Daenerys Targaryen SPOUSE Craster</td>\n",
       "      <td>2936</td>\n",
       "      <td>-0.535721</td>\n",
       "      <td>0.369183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>House Stark of Winterfell IN_REGION Dorne</td>\n",
       "      <td>2504</td>\n",
       "      <td>-0.311253</td>\n",
       "      <td>0.422809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Jaime Lannister PARENT_OF Myrcella Baratheon</td>\n",
       "      <td>2817</td>\n",
       "      <td>-0.266176</td>\n",
       "      <td>0.433846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Rhaegar Targaryen PARENT_OF Jon Snow</td>\n",
       "      <td>3374</td>\n",
       "      <td>-0.232269</td>\n",
       "      <td>0.442192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Daenerys Targaryen SPOUSE Jon Snow</td>\n",
       "      <td>2309</td>\n",
       "      <td>-0.100005</td>\n",
       "      <td>0.475020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Brienne of Tarth SPOUSE Jaime Lannister</td>\n",
       "      <td>1938</td>\n",
       "      <td>-0.039280</td>\n",
       "      <td>0.490181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Cersei Lannister PARENT_OF Myrcella Baratheon</td>\n",
       "      <td>1658</td>\n",
       "      <td>0.111919</td>\n",
       "      <td>0.527951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>House Tyrell of Highgarden IN_REGION Beyond th...</td>\n",
       "      <td>1180</td>\n",
       "      <td>0.196115</td>\n",
       "      <td>0.548872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>King's Landing SEAT_OF House Lannister of Cast...</td>\n",
       "      <td>723</td>\n",
       "      <td>0.590555</td>\n",
       "      <td>0.643493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Daenerys Targaryen ALLIED_WITH House Lannister...</td>\n",
       "      <td>228</td>\n",
       "      <td>1.385452</td>\n",
       "      <td>0.799865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Tywin Lannister PARENT_OF Jaime Lannister</td>\n",
       "      <td>75</td>\n",
       "      <td>1.526643</td>\n",
       "      <td>0.821515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sansa Stark SPOUSE Petyr Baelish</td>\n",
       "      <td>109</td>\n",
       "      <td>1.889842</td>\n",
       "      <td>0.868737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Robert I Baratheon PARENT_OF Myrcella Baratheon</td>\n",
       "      <td>18</td>\n",
       "      <td>1.951341</td>\n",
       "      <td>0.875593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>House Stark of Winterfell IN_REGION The North</td>\n",
       "      <td>12</td>\n",
       "      <td>2.354562</td>\n",
       "      <td>0.913296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Daenerys Targaryen ALLIED_WITH House Stark of ...</td>\n",
       "      <td>63</td>\n",
       "      <td>2.374144</td>\n",
       "      <td>0.914834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Missandei SPOUSE Grey Worm</td>\n",
       "      <td>65</td>\n",
       "      <td>3.164453</td>\n",
       "      <td>0.959474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Brandon Stark ALLIED_WITH House Stark of Winte...</td>\n",
       "      <td>1</td>\n",
       "      <td>6.629962</td>\n",
       "      <td>0.998681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>House Hutcheson SWORN_TO House Tyrell of Highg...</td>\n",
       "      <td>4</td>\n",
       "      <td>9.970955</td>\n",
       "      <td>0.999953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            statement  rank     score  \\\n",
       "10  Brandon Stark ALLIED_WITH House Lannister of C...  3998 -2.925931   \n",
       "18           Cersei Lannister PARENT_OF Brandon Stark  4079 -2.160664   \n",
       "0             Jorah Mormont SPOUSE Daenerys Targaryen  3319 -0.859091   \n",
       "1                   Tyrion Lannister SPOUSE Missandei  2977 -0.542991   \n",
       "5                   Daenerys Targaryen SPOUSE Craster  2936 -0.535721   \n",
       "7           House Stark of Winterfell IN_REGION Dorne  2504 -0.311253   \n",
       "15       Jaime Lannister PARENT_OF Myrcella Baratheon  2817 -0.266176   \n",
       "11               Rhaegar Targaryen PARENT_OF Jon Snow  3374 -0.232269   \n",
       "4                  Daenerys Targaryen SPOUSE Jon Snow  2309 -0.100005   \n",
       "21            Brienne of Tarth SPOUSE Jaime Lannister  1938 -0.039280   \n",
       "17      Cersei Lannister PARENT_OF Myrcella Baratheon  1658  0.111919   \n",
       "8   House Tyrell of Highgarden IN_REGION Beyond th...  1180  0.196115   \n",
       "2   King's Landing SEAT_OF House Lannister of Cast...   723  0.590555   \n",
       "14  Daenerys Targaryen ALLIED_WITH House Lannister...   228  1.385452   \n",
       "19          Tywin Lannister PARENT_OF Jaime Lannister    75  1.526643   \n",
       "3                    Sansa Stark SPOUSE Petyr Baelish   109  1.889842   \n",
       "16    Robert I Baratheon PARENT_OF Myrcella Baratheon    18  1.951341   \n",
       "6       House Stark of Winterfell IN_REGION The North    12  2.354562   \n",
       "13  Daenerys Targaryen ALLIED_WITH House Stark of ...    63  2.374144   \n",
       "20                         Missandei SPOUSE Grey Worm    65  3.164453   \n",
       "9   Brandon Stark ALLIED_WITH House Stark of Winte...     1  6.629962   \n",
       "12  House Hutcheson SWORN_TO House Tyrell of Highg...     4  9.970955   \n",
       "\n",
       "        prob  \n",
       "10  0.050886  \n",
       "18  0.103339  \n",
       "0   0.297529  \n",
       "1   0.367492  \n",
       "5   0.369183  \n",
       "7   0.422809  \n",
       "15  0.433846  \n",
       "11  0.442192  \n",
       "4   0.475020  \n",
       "21  0.490181  \n",
       "17  0.527951  \n",
       "8   0.548872  \n",
       "2   0.643493  \n",
       "14  0.799865  \n",
       "19  0.821515  \n",
       "3   0.868737  \n",
       "16  0.875593  \n",
       "6   0.913296  \n",
       "13  0.914834  \n",
       "20  0.959474  \n",
       "9   0.998681  \n",
       "12  0.999953  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.predict(X_unseen)\n",
    "\n",
    "# scores are real numbers that need to be translated into probabilities [0,1] \n",
    "# for this, we use the expit transform.\n",
    "\n",
    "from scipy.special import expit\n",
    "probs = expit(scores)\n",
    "\n",
    "pd.DataFrame(list(zip([' '.join(x) for x in X_unseen], \n",
    "                      ranks_unseen, \n",
    "                      np.squeeze(scores),\n",
    "                      np.squeeze(probs))), \n",
    "             columns=['statement', 'rank', 'score', 'prob']).sort_values(\"score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB : the probabilities are not calibrated in any sense. To calibrate them, one may use a procedure such as [Platt scaling](https://en.wikipedia.org/wiki/Platt_scaling) or [Isotonic regression](https://en.wikipedia.org/wiki/Isotonic_regression). The challenge is to define what is a true triple and what is a false one, as the calibration of the probability of a triple being true depends on the base rate of positives and negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Analyse the results in the tables. Some predicted links are very likely to be true, others  capture things that never really happened. Can you spot which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tensorboard](https://www.tensorflow.org/tensorboard) allows to dig into the workings of our model, plot how it is learning, and visualize [high-dimensional embeddings](https://projector.tensorflow.org/). See [this tutorial](https://www.tensorflow.org/tensorboard/get_started) to get started with Tensorflow. \n",
    "\n",
    "Lets import the <i>create_tensorboard_visualization</i> function, which simplifies the creation of the files necessary for Tensorboard to display the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.utils import create_tensorboard_visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can run the function with our model, specifying the output path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tensorboard_visualizations(model, './data/GoT_embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all went well, we should now have a number of files in the ./GoT_embeddings directory:\n",
    "\n",
    "```\n",
    "data/GoT_embeddings/\n",
    "    |---checkpoint\n",
    "    |--- embeddings_projector.tsv\n",
    "    |---graph_embedding.ckpt.data-00000-of-00001\n",
    "    |--- graph_embedding.ckpt.index\n",
    "    |--- graph_embedding.ckpt.meta\n",
    "    |--- metadata.tsv\n",
    "    |--- projector_config.pbtxt\n",
    "```\n",
    "    \n",
    "To visualise the embeddings in Tensorboard, run the following from your command line inside the tutorial folder:\n",
    "\n",
    "```code\n",
    "tensorboard --logdir=./data/GoT_embeddings\n",
    "```\n",
    ".. and once your browser opens up you should be able to see and explore your embeddings as below (PCA-reduced, two components):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard module is not an IPython extension.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./data/GoT_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7 Your Own Data now\n",
    "\n",
    "Choose a dataset of your own. Best if it is the data you are using in your group project. \n",
    "\n",
    "- Create a training and testset. \n",
    "- Train your model to compute Knowledge Graph Embeddings, and save the best parameters model. - Predict new links over your dataset\n",
    "- Visualise the embeddings you computed \n",
    "- Optional : cluster your embeddings, [see this tutorial](https://docs.ampligraph.org/en/1.4.0/tutorials/ClusteringAndClassificationWithEmbeddings.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
