{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KG measures and clustering tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, make sure you have networkx installed, here you can find the [documentation](https://networkx.org/documentation/stable/reference/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment if you do not have networkx installed (you should have it installed from the RDFS tutorial)\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install networkx\n",
    "\n",
    "import pandas as pd\n",
    "import rdflib\n",
    "from rdflib import Graph, Literal, Namespace, RDF, URIRef, OWL\n",
    "from rdflib.extras.external_graph_libs import rdflib_to_networkx_digraph\n",
    "from rdflib.namespace import DC, FOAF\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx.algorithms.community as nx_comm\n",
    "\n",
    "import networkx as nx\n",
    "from owlready2 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will focus on how to characterize an ontology or knowledge graph.\n",
    "\n",
    "Load the ontology, which you have previously created in the OWL tutorial (load the asserted owl file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ontology = rdflib.Graph()\n",
    "ontology.parse(\"../data/my_music_ontology_inferred.owl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic (ontology) measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first focus on calculating basic measures:\n",
    "* number of classes\n",
    "* number of properties\n",
    "* number of individuals\n",
    "* number of triples\n",
    "* number of entities (classes, individuals, etc. anything that can be places in the subject possition in a triple/axiom)\n",
    "\n",
    "We start by counting the number of classes in the ontology. This can be done using a SPARQL query. We want to get the unique classes used in the ontology. \n",
    "\n",
    "PREFIX owl: <http://www.w3.org/2002/07/owl#> \n",
    "SELECT DISTINCT ?s \n",
    "WHERE { ?s rdf:type owl:Class. FILTER isURI(?s) }')\n",
    "\n",
    "This query gives as all the classes that also have a definition in the ontology. However, this does not have to equal the number of classes actually used by individuals. Hence, you need to be very specific about what the number you are retriving represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = list(ontology.query(\n",
    "    'PREFIX owl: <http://www.w3.org/2002/07/owl#> SELECT DISTINCT ?s WHERE { ?s rdf:type owl:Class. FILTER isURI(?s) }'\n",
    "))\n",
    "print(\"Number of classes: {f}\".format(f=len(answer)))\n",
    "for r in answer:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we used a query, a lot of this information can also be retrieved with owlready2. For example, the number of classes can be retrieved with the function onto.classes(). It returns all classes in the ontology. We try it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -al\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "onto_file = \"../data/my_music_ontology_inferred.owl\"\n",
    "or_ontology = get_ontology(onto_file).load()\n",
    "answer = list(or_ontology.classes())\n",
    "\n",
    "print(\"Number of classes: {f}\".format(f=len(answer)))\n",
    "for r in answer:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = list(or_ontology.individuals())\n",
    "print(len(ind))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Get the following metrics from your ontology, using queries and check your answer using owlready2 functions.\n",
    "\n",
    "* number of properties\n",
    "* number of individuals\n",
    "* number of triples\n",
    "* number of entities (classes, individuals, etc. anything that can be places in the subject possition in a triple/axiom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here.\n",
    "\n",
    "#number of properties\n",
    "answer = list(ontology.query(\n",
    "    'PREFIX owl: <http://www.w3.org/2002/07/owl#> SELECT DISTINCT ?s WHERE { ?s rdf:type owl:ObjectProperty. FILTER isURI(?s) }'\n",
    "))\n",
    "print(\"Number of properties: {f}\".format(f=len(answer)))\n",
    "for r in answer:\n",
    "    print(r)\n",
    "\n",
    "\n",
    "#number of individuals\n",
    "answer = list(ontology.query(\n",
    "    'PREFIX owl: <http://www.w3.org/2002/07/owl#> SELECT DISTINCT ?s WHERE { ?s rdf:type owl:NamedIndividual. FILTER isURI(?s) }'\n",
    "))\n",
    "print(\"Number of Named Individuals: {f}\".format(f=len(answer)))\n",
    "for r in answer:\n",
    "    print(r)\n",
    "\n",
    "\n",
    "#numner of triples\n",
    "answer = list(ontology.query(\n",
    "    'PREFIX owl: <http://www.w3.org/2002/07/owl#> SELECT ?s WHERE { ?s ?p ?o. FILTER isURI(?s) } '\n",
    "))\n",
    "print(\"Number of Triples: {f}\".format(f=len(answer)))\n",
    "#for r in answer:\n",
    "#    print(r)\n",
    "\n",
    "\n",
    "#number of entities aka anything that can be placed in subject position of triples\n",
    "answer = list(ontology.query(\n",
    "    'PREFIX owl: <http://www.w3.org/2002/07/owl#> SELECT ?s (count(?s) AS ?count) WHERE { ?s ?p ?o . FILTER isURI(?s) } group by ?s order by desc(?count)'\n",
    "))\n",
    "print(\"Number of entities: {f}\".format(f=len(answer)))\n",
    "for r in answer:\n",
    "    print(r)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Converting KGs into Gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make use of graph measure, we need to convert our ontology into a mathematical graph networkx.\n",
    "\n",
    "\n",
    "We first need to remove all the logics before we can do the conversion.\n",
    "We are interested in keeping the following things:\n",
    "* individual\n",
    "* classes\n",
    "* relationships between individuals and classes\n",
    "\n",
    "What we need to remove is:\n",
    "* restrictions\n",
    "* domain/range\n",
    "* property definitions\n",
    "\n",
    "There is two ways for us to do it: we can either remove the information from the existing graph, or create a new graph using only the information we are interested in. Depending on the size and complexity of your knowledge graph, one way will be more preferrable than the other. You also need to consider if you want to keep the inferred information in your graph after conversion or not. Here, we want to keep the inferred information, but that is depended on the task you will then execute (for link prediction, you probably want the uninferred ontology and use the inferred information as a test set)\n",
    "\n",
    "rdflib comes with a function that lets us convert a rdflib graph into an networkx graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib.extras.external_graph_libs import rdflib_to_networkx_digraph\n",
    "nx_graph = rdflib_to_networkx_digraph(ontology)\n",
    "\n",
    "list(nx_graph.nodes())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are some blank nodes which were covered into the graph that are not very useful for us at this stage. To analyse the graph as a mathematical graph, we don't want the class restrictions or property range and domain definition in our graph, as we are not doing any reasoning anymore.\n",
    "\n",
    "Often times, it is easier to create a new graph than removeing already modeled information from the graph. Instead of continuing with the ontology, we will create a graph from the metadata provided in 'data/musicoset_metadata', but will adhere to the ontology from before (use the property and class names,etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "csv_albums =  pd.read_csv('../data/musicoset_metadata/albums.csv',sep='\\t')\n",
    "print(csv_albums.columns)\n",
    "csv_artists =  pd.read_csv('../data/musicoset_metadata/artists.csv',sep='\\t')\n",
    "print(csv_artists.columns)\n",
    "csv_songs =  pd.read_csv('../data/musicoset_metadata/songs.csv',sep='\\t')\n",
    "print(csv_songs.columns)\n",
    "csv_tracks =  pd.read_csv('../data/musicoset_metadata/tracks.csv',sep='\\t')\n",
    "print(csv_tracks.columns)\n",
    "\n",
    "csv_releases =  pd.read_csv('../data/musicoset_metadata/releases.csv',sep='\\t')\n",
    "print(csv_releases.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have prepared a simplified ontology to use in this tutorial\n",
    "# This ontology doesn't have any restrictions or domain/range definitions\n",
    "# this is to avoid blank nodes when converting to networx\n",
    "music_onto = rdflib.Graph()\n",
    "music_onto.parse(\"../data/music_onto_simple.rdf\")\n",
    "\n",
    "nx_music = rdflib_to_networkx_digraph(music_onto)\n",
    "list(nx_music.nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ontology no longer produces any blank nodes. So we can now populate it with the metadata loaded from the CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_onto.parse(\"../data/music_onto_simple.rdf\")\n",
    "\n",
    "EX = rdflib.Namespace(\"http://test.org/myonto.owl#\")\n",
    "from rdflib import OWL,RDF,RDFS,URIRef\n",
    "import json\n",
    "solo_artists = [\"singer\",'rapper','DJ',]\n",
    "band = ['band','duo']\n",
    "undef = ['-']\n",
    "\n",
    "for index, artist in csv_artists.iterrows():\n",
    "    art = URIRef(EX,artist[\"artist_id\"])\n",
    "    if artist[\"artist_type\"] in solo_artists:\n",
    "        music_onto.add((art,RDF.type,EX.SoloArtist))\n",
    "    elif artist[\"artist_type\"] in band:\n",
    "        music_onto.add((art,RDF.type,EX.Band))\n",
    "    else:\n",
    "        music_onto.add((art,RDF.type,EX.Artist))\n",
    "        \n",
    "    music_onto.add((art,EX.name,Literal(artist[\"name\"])))\n",
    "    music_onto.add((art,EX.followers,Literal(artist[\"followers\"])))\n",
    "    genre = URIRef(EX,artist[\"main_genre\"])\n",
    "    music_onto.add((art,EX.hasGenre,genre))\n",
    "    music_onto.add((genre,RDF.type,EX.Genre))\n",
    "    \n",
    "for index, song in csv_songs.iterrows():\n",
    "    s = URIRef(EX,song[\"song_id\"])\n",
    "    music_onto.add((s,RDF.type,EX.Song))\n",
    "    music_onto.add((s,EX.name,Literal(song['song_name'])))\n",
    "    artists = eval(song['artists'])\n",
    "    for key in artists.keys():\n",
    "        art = URIRef(EX,key)\n",
    "        music_onto.add((art,EX.authorOf,s))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to extend your ontology to include albums and track information below. You might have to extend the basic ontology as well with some additional relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Expanding the ONTOLOGY\n",
    "EX = rdflib.Namespace(\"http://test.org/myonto.owl#\")\n",
    "music_onto.add((EX.Album, RDF.type, OWL.Class))\n",
    "music_onto.add((EX.HasTotalTracks, RDF.type, OWL.ObjectProperty))\n",
    "\n",
    "#saving new ontology \n",
    "nx_music_onto = rdflib_to_networkx_digraph(music_onto)\n",
    "#print(list(nx_music_onto))\n",
    "\n",
    "#Adding albums and author of relation\n",
    "for index, album in csv_albums.iterrows():\n",
    "    \n",
    "    alb = URIRef(EX,album[\"album_id\"])\n",
    "    art =  URIRef(EX,album[\"artists\"])\n",
    "    music_onto.add((alb, RDF.type, EX.Album))\n",
    "    music_onto.add((alb,EX.name,Literal(album['name'])))\n",
    "\n",
    "    music_onto.add((Literal(album['artists']),EX.authorOf, Literal(album['name'])))\n",
    "\n",
    "\n",
    "#Adding release date to album\n",
    "for index, album in csv_releases.iterrows():\n",
    "    alb = URIRef(EX,album[\"album_id\"])\n",
    "    release_date = URIRef(EX,album[\"release_date\"])\n",
    "    music_onto.add((alb, EX.releaseDate,Literal(album['release_date'])))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert the ontology to a networkx graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nx_music_graph = rdflib_to_networkx_digraph(music_onto)\n",
    "#By printing the nodes you can double check your ontology updates\n",
    "list(nx_music_graph.nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "* Convert your knowledge graph or ontology into a networkx graph. \n",
    "* Write a function that checks for blank nodes in your networkx graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def find_Blank_nodes(nx_obj):\n",
    "    blank_nodes = []\n",
    "    count = 0\n",
    "    for i in range(len(list(nx_obj.nodes()))):\n",
    "        text = str(list(nx_obj.nodes())[i])\n",
    "        #if node has URI is not blank\n",
    "        if re.search('http',text):\n",
    "\n",
    "            count += 0\n",
    "        #if node has not URI is a blank node    \n",
    "        else: \n",
    "            count += 1\n",
    "            blank_nodes.append(text)\n",
    "\n",
    "    print(f'There are {count} BlankNodes. \\n And they are:\\n {blank_nodes}')\n",
    "    return count, blank_nodes\n",
    "\n",
    "\n",
    "#check if it works: in the inferred ontology there should be some Blank Nodes\n",
    "print('music KG')\n",
    "count, blank_nodes = find_Blank_nodes(nx_graph)\n",
    "\n",
    "\n",
    "\n",
    "#check if it works: on our ontology there should be None\n",
    "print('music ontology')\n",
    "count, blank_nodes = find_Blank_nodes(nx_music)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Graph Measures\n",
    "\n",
    "Now we can calculate some graph measures over the networkX graph. The library provides a lot of different measures that can be calculated. Always check what kind of assumptions the measure has:\n",
    "* directed or undirected graph?\n",
    "* does the graph have to be connected?\n",
    "\n",
    "We will first calculate some basic graph measures: number of nodes, number of edges and the density of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Nodes: {n}\".format(n=nx.number_of_nodes(nx_music_graph)))\n",
    "print(\"Number of Edges: {n}\".format(n=nx.number_of_edges(nx_music_graph)))\n",
    "print(\"Density of Graph: {n}\".format(n=nx.density(nx_music_graph)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now look at the distribution of degree of our nodes by calculating (retrieving) the degree for each not and plotting a histogram.\n",
    "Additionally, we also want to know the averge degree centrality of our graph and put it in the title of our histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "histdegree = pd.DataFrame(nx.degree_histogram(nx_graph))\n",
    "degree = dict(nx.degree(nx_graph))\n",
    "\n",
    "\n",
    "mean_degree = np.mean(list(degree.values()))\n",
    "mean_degree_centrality = np.mean(list(nx.degree_centrality(nx_graph).values()))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6)) \n",
    "ax.bar(histdegree.index.values,histdegree[0])\n",
    "\n",
    "plt.title(\"Mean Degree: {n1}\\n Mean Degree Centrality: {n2}\".format(n1=mean_degree,n2=mean_degree_centrality))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another very useful measure is the clustering coefficient, which tells us how likely the nodes are to build clusters. This is a global measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Clustering coefficient: {n}\".format(n=nx.average_clustering(nx_graph)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Calculate and visualise the centrality of the music graph. Use a different measure than degree. For different measures you can refer to this online [documentation]{https://networkx.org/documentation/stable/reference/algorithms/centrality.html}. Choose wisely though, some measures require a long time to calculate (like betweenness or eigenvector centrality).\n",
    "\n",
    "As a second step, take some time to explore the documentation of networkx. Is there something other you can calculate and learn about the graph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "#here we are plotting the ontology of the music graph\n",
    "histdegree = pd.DataFrame(nx.degree_histogram(nx_music))\n",
    "degree = dict(nx.degree(nx_music))\n",
    "histdegree = histdegree[:10]\n",
    "\n",
    "mean_degree = np.mean(list(degree.values()))\n",
    "mean_degree_centrality = np.mean(list(nx.degree_centrality(nx_music).values()))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6)) \n",
    "ax.bar(histdegree.index.values[:10],histdegree[0])\n",
    "\n",
    "plt.title(\"Mean Degree: {n1}\\n Mean Degree Centrality: {n2}\".format(n1=mean_degree,n2=mean_degree_centrality))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we are plotting the music graph:\n",
    "##we can clearly see that there is a big imbalabce as only the first two degreeS are significantly bigger than zero\n",
    "histdegree = pd.DataFrame(nx.degree_histogram(nx_music_graph))\n",
    "degree = dict(nx.degree(nx_music_graph))\n",
    "histdegree = histdegree[:10]\n",
    "\n",
    "mean_degree = np.mean(list(degree.values()))\n",
    "mean_degree_centrality = np.mean(list(nx.degree_centrality(nx_music_graph).values()))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6)) \n",
    "ax.bar(histdegree.index.values[:10],histdegree[0])\n",
    "\n",
    "plt.title(\"Mean Degree: {n1}\\n Mean Degree Centrality: {n2}\".format(n1=mean_degree,n2=mean_degree_centrality))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can for instance compute the degree centrality: which is the fraction of the nodes it is connected to \n",
    "print(nx.degree_centrality(nx_music))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can for instance compute the in-degree centrality: which is the fraction of nodes its incoming edges are connected to.\n",
    "nx.in_degree_centrality(nx_music)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing your graph\n",
    "\n",
    "With networkX you can easily visualize your ontology/graph, no matter if they include blank nodes or not. The visualisations are powered by matplotlib. We will use here the first ontology, which also has blank nodes, but is mucy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working with original ontology inferred - even tho it contains Blank nodes\n",
    "\n",
    "music_ontology = rdflib.Graph()\n",
    "music_ontology.parse(\"../data/my_music_ontology_inferred.owl\")\n",
    "nx_graph = rdflib_to_networkx_digraph(music_ontology)\n",
    "\n",
    "#rename the columns\n",
    "mapping = pd.DataFrame(nx_graph.nodes())\n",
    "mapping['new_names'] = mapping[0].str.split(\"#\",n=1,expand=False)\n",
    "mapping['label'] = 'NA'\n",
    "\n",
    "mapping_copy = mapping.copy()\n",
    "\n",
    "for ind, m in mapping_copy.iterrows():\n",
    "    l = len(m['new_names'])\n",
    "    names = m['new_names']\n",
    "    mapping.loc[ind,'label'] = names[l-1]\n",
    "#this just helps us to have the readable names and not the URIs    \n",
    "map_dict = dict(zip(mapping[0],mapping['label']))\n",
    "\n",
    "#we indeed re-label the nodes of the graph\n",
    "nx_graph_nl = nx.relabel_nodes(nx_graph,map_dict,copy=True)\n",
    "\n",
    "#we decide to color the owl properties (in this way we have the ontology interacting with the instantiated KG)\n",
    "owl_classes = [str(mapping.new_names[i][1])  for i in range(len(mapping.new_names)) if re.search('http://www.w3.org/2002/07/',mapping.new_names[i][0] )]\n",
    "\n",
    "#this is quite messy as our KG is pretty big and networkx visualization tools are not known to be the best around\n",
    "nx.draw(nx_graph_nl,\n",
    "                 with_labels=True, nodelist = owl_classes,node_color = [range(11)],\n",
    "                node_size=200, font_size=8,alpha=0.6,linewidths=0.1,style=':' )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Try to visualize your own ontology below. Don't shy away from looking for and trying other approaches to visualize an ontology, giving nodes different colors, or varying thickness/color of the edges based on the type of relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we visualize the ontology that we enlarged with albums and all\n",
    "\n",
    "mapping = pd.DataFrame(nx_music.nodes())\n",
    "mapping['new_names'] = mapping[0].str.split(\"#\",n=1,expand=False)\n",
    "\n",
    "mapping['label'] = 'NA'\n",
    "\n",
    "#store a list of the owl classes: we will color them differently for better visualization\n",
    "owl_classes = [str(mapping.new_names[i][1])  for i in range(len(mapping.new_names)) if re.search('http://www.w3.org/2002/07/',mapping.new_names[i][0] )]\n",
    "\n",
    "\n",
    "mapping_copy = mapping.copy()\n",
    "\n",
    "for ind, m in mapping_copy.iterrows():\n",
    "    l = len(m['new_names'])\n",
    "    names = m['new_names']\n",
    "    mapping.loc[ind,'label'] = names[l-1]\n",
    "    \n",
    "map_dict = dict(zip(mapping[0],mapping['label']))\n",
    "\n",
    "nx_graph_nl = nx.relabel_nodes(nx_music,map_dict,copy=True)\n",
    "\n",
    "print(nx_graph_nl)\n",
    "\n",
    "\n",
    "nx.draw_planar(nx_graph_nl,\n",
    "                 with_labels=True,\n",
    "                node_size=800, font_size=9 , font_color = \"darkblue\",nodelist = owl_classes,node_color = [range(len(owl_classes))], alpha=0.6,linewidths=0.1,style=':')\n",
    "# plt.draw()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clustering\n",
    "\n",
    "NetworkX already comes with some clustering algorithms. We will try the one introduced in the theory part of the class, Louvain clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#work with the original inferred ontology\n",
    "\n",
    "music_ontology = rdflib.Graph()\n",
    "music_ontology.parse(\"../data/my_music_ontology_inferred.owl\")\n",
    "nx_graph = rdflib_to_networkx_digraph(music_ontology)\n",
    "\n",
    "mapping = pd.DataFrame(nx_graph.nodes())\n",
    "mapping['new_names'] = mapping[0].str.split(\"#\",n=1,expand=False)\n",
    "mapping['label'] = 'NA'\n",
    "\n",
    "mapping_copy = mapping.copy()\n",
    "\n",
    "for ind, m in mapping_copy.iterrows():\n",
    "    l = len(m['new_names'])\n",
    "    names = m['new_names']\n",
    "    mapping.loc[ind,'label'] = names[l-1]\n",
    "#this just helps us to have the readable names and not the URIs    \n",
    "map_dict = dict(zip(mapping[0],mapping['label']))\n",
    "\n",
    "nx_graph_nl = nx.relabel_nodes(nx_graph,map_dict,copy=True)\n",
    "\n",
    "#we print all the communities to get a clearer idea of whatsuppp\n",
    "communities = nx_comm.louvain_communities(nx_graph_nl,resolution=1) #resolution: high number favor small communitites, low favor large communities\n",
    "print('Number of found communitites', len(communities),\n",
    "'\\n Number of nodes in the graph',nx.number_of_nodes(nx_graph_nl))\n",
    "for i in range(len(communities)):\n",
    "    print(f'{i}th community: \\n',communities[i] )\n",
    "    \n",
    "\n",
    "pos = nx.shell_layout(nx_graph_nl)\n",
    "nx.draw(nx_graph_nl, pos, edge_color='k',font_weight='light', \n",
    "        node_size= 100, width= 0.8)\n",
    "\n",
    "for com in communities:\n",
    "       nx.draw_networkx_nodes(nx_graph_nl,\n",
    "                           pos,\n",
    "                           nodelist=com, \n",
    "                           #node_color=np.random.rand(3,),\n",
    "                           node_color = range(len(com)),\n",
    "                           label=True,\n",
    "                           node_size=100)    \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for com in communities:\n",
    "    nx.draw(nx_graph_nl, with_labels=True,\n",
    "                node_size=800, font_size=9 , font_color = \"darkblue\",nodelist = com, node_color=range(len(com)),alpha=0.6,linewidths=0.1,style=':')\n",
    "plt.draw()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that visualizing all together gets quite messy: let's visualize community by community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(nx_graph_nl, with_labels=True,\n",
    "                node_size=800, font_size=5 , font_color = \"darkblue\",nodelist = communities[1],alpha=0.6,linewidths=0.1,style=':')\n",
    "plt.draw()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization is still quite messy as there are many nodes to work with.\n",
    "Here we do cluster and visualization of the ontology only (which has indeed less nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#work with music ontology only\n",
    "import networkx.algorithms.community as nx_comm\n",
    "#Find the communities\n",
    "nx_graph_nl = nx.relabel_nodes(nx_music,map_dict,copy=True)\n",
    "communities = nx_comm.louvain_communities(nx_graph_nl,resolution=1) #resolution: high number favor small communitites, low favor large communities\n",
    "print('Number of found communitites', len(communities),\n",
    "'\\n Number of nodes in the graph',nx.number_of_nodes(nx_graph_nl))\n",
    "\n",
    "#Print the member of each community: can try and reason on the semantic value of that\n",
    "for i in range(len(communities[:10])):\n",
    "    print(f'{i}th community: \\n',communities[i] )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize the graph and the communities within it. Alternatively you can also visualize a community as an example alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.shell_layout(nx_graph_nl)\n",
    "nx.draw(nx_graph_nl, pos, edge_color='k',font_weight='light', \n",
    "        node_size= 100, width= 0.8)\n",
    "\n",
    "for com in communities:\n",
    "       nx.draw_networkx_nodes(nx_graph_nl,\n",
    "                           pos,\n",
    "                           nodelist=com, \n",
    "                           node_color=range(len(com)),\n",
    "                           label=True,\n",
    "                           node_size=100)\n",
    "\n",
    "\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for com in communities:\n",
    "    nx.draw_planar(nx_graph_nl, with_labels=True,\n",
    "                node_size=800, font_size=9 , font_color = \"darkblue\",nodelist = com, node_color=range(len(com)),alpha=0.6,linewidths=0.1,style=':')\n",
    "# plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can highlight each community at a time - change the value of parameter nodelist\n",
    "\n",
    "nx.draw_planar(nx_graph_nl, with_labels=True,\n",
    "                node_size=800, font_size=9 , font_color = \"darkblue\",nodelist = communities[1],alpha=0.6,linewidths=0.1,style=':')\n",
    "# plt.draw()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If clustering is successful and helpful is very dependent on your graph. It is beneficial to remove the information from your graph that is not helpful for the clustering, like the T-box of your ontology, as this \"polutes\" the graph. The clustering algorithm is not made for knowledge graphs but rather for mathematical graphs, hence less semantics is better.\n",
    "\n",
    "In the case of the inferred ontology (nx_graph), the graph is not dense enough to produce meaningful clusters, which is why there are 38706 clusters. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c30f2af5f468e7f5b45bcc30fca5f4886c90d54777aed916ed5f6294dfb24bf2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
